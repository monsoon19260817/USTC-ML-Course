%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages} 
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{ctex}
\usepackage{diagbox}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Define math operator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\relint}{\bf relint\,}
\DeclareMathOperator*{\dom}{\bf dom\,}
\DeclareMathOperator*{\intp}{\bf int\,}
%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Problem environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{problem}{\textbf{Problem}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Oct. 21, 2019}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Oct. 30, 2019}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{3}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Jiahuan Yu}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB17121687}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
	{\bf\large Introduction to Machine Learning}\\
	{Fall 2019}\\
	University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name
\hfill
ID: \id
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please present your solutions step by step.

\begin{exercise}[Logistic Regression \textnormal{40pts}]
	Given the training data $\mathcal{D}=\{ (\textbf{x}_i,y_i) \}_{i=1}^n$, where $\textbf{x}_i \in \mathbb{R}^d$ and $y_i \in \{ 0,1 \}$. Let
	\begin{align*}
		\mathcal{I}^+= & \{i:i\in[n],y_i=1\}, \\
		\mathcal{I}^-= & \{i:i\in[n],y_i=0\},
	\end{align*}
	where $[n]=\{1,2,\ldots,n\}$. We assume that $\mathcal{I}^+$ and $\mathcal{I}^-$ are not empty.

	Then, we can formulate the logistic regression as:
	\begin{equation}\label{prob:logistic}
		\min_{\textbf{w}}\,\,L(\textbf{w})=-\frac{1}{n}\sum_{i=1}^n \left( y_i \log ( \frac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } ) + (1-y_i)\log ( \frac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)} ) \right),
	\end{equation}
	where $\mathbf{w} \in \mathbb{R}^{d+1}$ is the model parameter to be estimated and $ \overline{\mathbf{x}}_i^{\top} = (1,\mathbf{x}_i^{\top}) $.

	\begin{enumerate}
		\item Find the gradient and the Hessian of $L(\textbf{w})$.
		\item Suppose that $\overline{\textbf{X}}=(\overline{\mathbf{x}}_1,\overline{\mathbf{x}}_2,\dots,\overline{\mathbf{x}}_n)^\top\in\mathbb{R}^{n \times (d+1)}$ and $\rank{\overline{\mathbf{X}}}=d+1$. Show that $L(\textbf{w})$ is strictly convex, i.e., for all $\textbf{w}_1\neq \textbf{w}_2$,
		      \begin{align*}
			      L(t\textbf{w}_1 + (1-t)\textbf{w}_2) < t L(\textbf{w}_1)+(1-t)L(\textbf{w}_2),\forall\, t \in (0,1).
		      \end{align*}
		\item Suppose that the training data is linearly separable, that is, there exists $\hat{\mathbf{w}}\in\mathbb{R}^{d+1}$ such that
		      \begin{align*}
			       & \langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle>0,\,\forall\,i\in\mathcal{I}^+, \\
			       & \langle \hat{\mathbf{w}}, \mathbf{\bar{x}}_i\rangle<0,\,\forall\,i\in\mathcal{I}^-.
		      \end{align*}
		      Show that problem (\ref{prob:logistic}) has no solution.
		\item (\textbf{Bonus} 20pts) Suppose that the training data is NOT linearly separable. Show that problem (\ref{prob:logistic}) always admits a solution. Moreover, show that the solution is unique.
	\end{enumerate}
\end{exercise}
\begin{solution}
	\begin{enumerate}
		\item $$\begin{aligned}
				      \nabla L(\mathbf{w})
				       & = -\cfrac{1}{n}\sum_{i=1}^{n}{\left( y_i \cfrac{\nabla \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) }  }{ \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) }  }   +(1-y_i) \cfrac{\nabla \cfrac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) }}{\cfrac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } } \right)} \\
				       & =-\cfrac{1}{n}\sum_{i=1}^{n}{ \left(
				      \cfrac{y_i}{ \exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\left(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\right)}
				      - \cfrac{1-y_i}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}
				      \right) \nabla \exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\
				       & =-\cfrac{1}{n}\sum_{i=1}^{n}{ \left(
				      \cfrac{y_i}{ \exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\left(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\right)}
				      - \cfrac{1-y_i}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}
				      \right) \exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)} \overline{\mathbf{x}}_i                                                                                                                                                                                                                                                                                                                                                                                                                                 \\
				       & = -\cfrac{1}{n}\sum_{i=1}^{n}{
				      \left(
				      y_i - \cfrac{\exp(\langle \textbf{w},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},\overline{\mathbf{x}}_i \rangle)}
				      \right)
				      \overline{\mathbf{x}}_i
				      }
			      \end{aligned}$$
		      设

		      $$L_i(\mathbf{w})=y_i \log \left( \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } \right) + (1-y_i)\log \left( \cfrac{1}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)} \right)$$

		      则 $\nabla L_i(\mathbf{w})$ 的第 $p$ 个分量为

		      $$\left(\nabla L_i(\mathbf{w})\right)_p=
			      \left(
			      y_i - \cfrac{\exp(\langle \textbf{w},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},\overline{\mathbf{x}}_i \rangle)}
			      \right) \overline{x}_{i,p}
			      =\cfrac{\partial L_i(\mathbf{w})}{\partial w_p}
		      $$

		      其中 $\overline{x}_{i,p}$ 是 $\overline{\mathbf{x}}_{i}$ 的第 $p$ 个分量。所以

		      $$\begin{aligned}
				      \cfrac{\partial^2 L_i(\mathbf{w})}{\partial w_p \partial w_q}
				       & =-\overline{x}_{i,p}\cfrac{\partial}{\partial w_q} \left( \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle) } \right)                                                               \\
				       & =-\overline{x}_{i,p}\cdot\cfrac{1}{\left(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\right)^2}\cdot\cfrac{\partial \exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{\partial w_q}                                                     \\
				       & = -\overline{x}_{i,p}\cdot\cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{\left(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\right)^2}\cdot\cfrac{\partial \langle \textbf{w},  \overline{\mathbf{x}}_i \rangle}{\partial w_q} \\
				       & =-\cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{\left(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)\right)^2}\cdot\overline{x}_{i,p}\overline{x}_{i,q}
			      \end{aligned}$$
		      所以
		      $$\begin{aligned}
				      \mathbf{H}(L(\mathbf{w}))
				       & = -\cfrac{1}{n}\sum_{i=1}^{n}{
					      \begin{pmatrix}
						      \cfrac{\partial^2 L_i(\mathbf{w})}{\partial w_1\partial w_1 } & \cdots & \cfrac{\partial^2 L_i(\mathbf{w})}{\partial w_1\partial w_n } \\
						      \vdots                                                        &        & \vdots                                                        \\
						      \cfrac{\partial^2 L_i(\mathbf{w})}{\partial w_n\partial w_1 } & \cdots & \cfrac{\partial^2 L_i(\mathbf{w})}{\partial w_n\partial w_n }
					      \end{pmatrix}
				      }                                      \\
				       & = \cfrac{1}{n}\sum_{i=1}^{n}{\left(
				      \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle))^2}\cdot\overline{\mathbf{x}}_{i}\overline{\mathbf{x}}_{i}^\top
				      \right)}
			      \end{aligned}$$
		\item $\forall \mathbf{y} \in \mathbb{R}^{(d+1)}$, $\|\mathbf{y}\|\neq0$, 有
		      $$
			      \mathbf{y}^\top\cdot \mathbf{H}\cdot \mathbf{y}
			      = \cfrac{1}{n}\sum_{i=1}^{n}{\left(
			      \cfrac{\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle)}{(1+\exp(\langle \textbf{w},  \overline{\mathbf{x}}_i \rangle))^2}\cdot \mathbf{y}^\top\overline{\mathbf{x}}_{i}\overline{\mathbf{x}}_{i}^\top\mathbf{y}
			      \right)}
		      $$
		      而
		      $$\mathbf{y}^\top\overline{\mathbf{x}}_{i}\overline{\mathbf{x}}_{i}^\top\mathbf{y}=\left(\overline{\mathbf{x}}_{i}^\top\mathbf{y}\right)^2$$
		      假设对 $\forall i =1,2,\cdots,n$, $\overline{\mathbf{x}}_{i}^\top\mathbf{y}=0$.

		      因为 $\rank{\overline{\mathbf{X}}}=d+1$，则方程组
		      $$\begin{pmatrix}
				      \overline{\mathbf{x}}_{1}^\top \\
				      \overline{\mathbf{x}}_{2}^\top \\
				      \vdots                         \\
				      \overline{\mathbf{x}}_{n}^\top
			      \end{pmatrix}\cdot \mathbf{y}=0$$
		      只有 $\mathbf{0}$ 解，与 $\mathbf{y}$ 的任意性相矛盾。

		      因此存在 $i$ 使得 $\overline{\mathbf{x}}_{i}^\top\mathbf{y}\neq0$.

		      所以 $\mathbf{y}^\top\cdot \mathbf{H}\cdot \mathbf{y}>0$.

		      所以 $\mathbf{H}$ 正定，所以 $L(\mathbf{w})$ 是凸函数。
		\item 设 $\lambda>0$, 则
		      $$
			      L(\lambda \hat{\mathbf{w}})
			      =-\frac{1}{n}\sum_{i=1}^n \left( y_i \log \left( \frac{\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle) } \right) + (1-y_i)\log \left( \frac{1}{1+\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle)} \right) \right)
		      $$
		      $$\begin{aligned}
				      \cfrac{\partial L(\lambda \hat{\mathbf{w}})}{\partial \lambda}
				       & = -\cfrac{1}{n}\sum_{i=1}^{n}{
				      \left( \left(
					      y_i - \cfrac{\exp(\langle \lambda \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \lambda \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}
					      \right)
				      \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle \right)
				      }                                 \\
				       & =-\cfrac{1}{n}\sum_{i=1}^{n}{
				      \left( \left(
					      y_i - \cfrac{\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}
					      \right)
				      \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle \right)
				      }
			      \end{aligned}$$
		      \begin{enumerate}
			      \item $y_i=0$ 时，$\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle<0$, 所以 $\left(
				            y_i - \cfrac{\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}
				            \right)
				            \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle>0$.
			      \item $y_i=1$ 时，$\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle>0$, 所以 $\left(
				            y_i - \cfrac{\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\lambda\langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle)}
				            \right)
				            \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle>0$.
		      \end{enumerate}
		      所以 $\cfrac{\partial L(\lambda \hat{\mathbf{w}})}{\partial \lambda}<0$.

		      又因为



		      $$\begin{aligned}
				       & \ \ \ \lim_{\lambda\to\infty}{L(\lambda \hat{\mathbf{w}})}                                                                                                                                                                                                                                                                                                                                      \\
				       & =-\frac{1}{n}\sum_{i=1}^n \left( y_i \log \lim_{\lambda\to\infty} \left( \frac{\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle) } \right) + (1-y_i)\log \lim_{\lambda\to\infty} \left( \frac{1}{1+\exp(\langle \lambda \hat{\mathbf{w}},  \overline{\mathbf{x}}_i \rangle)} \right) \right) \\
				       & = -\frac{1}{n}\sum_{i=1}^n \left( y_i \log 1  + (1-y_i)\log 1  \right)                                                                                                                                                                                                                                                                                                                          \\
				       & =0
			      \end{aligned}$$
		      同时 $L(\mathbf{w})>0$ 恒成立，可知 $\min L(\mathbf{w})$ 无法取到。
		\item 如果数据集满足 $\exists \hat{\mathbf{w}}\in\mathbb{R}^{(d+1)}$, 使得
		      $$\begin{aligned}
				      \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle & \ge 0, \forall i \in \mathcal{I}^+ \\
				      \langle \hat{\mathbf{w}},\overline{\mathbf{x}}_i \rangle & \le 0, \forall i \in \mathcal{I}^-
			      \end{aligned}$$
		      用上一题的方法，易证问题 (\ref{prob:logistic}) 无解。

		      所以只要考虑等号不能取到的情况。

		      由题意可知，对 $\forall \mathbf{w}\in \mathbb{R}^{(d+1)}$, $\exists \overline{\mathbf{x}}_i, i\in\{1,2,\cdots,n\}$, 使得
		      $$\begin{aligned}
				      \langle \mathbf{w},\overline{\mathbf{x}}_i \rangle & < 0, \text{if}\ i \in \mathcal{I}^+ \\
				      \langle \mathbf{w},\overline{\mathbf{x}}_i \rangle & > 0, \text{if}\ i \in \mathcal{I}^-
			      \end{aligned}$$
		      对于满足上述条件的 $\mathbf{x}_i$, 任取 $\lambda>0$, 有
		      $$\begin{aligned}
				      \langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle & = \lambda\langle \mathbf{w},\overline{\mathbf{x}}_i \rangle < 0, \text{if}\ i \in \mathcal{I}^+ \\
				      \langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle & = \lambda\langle \mathbf{w},\overline{\mathbf{x}}_i \rangle> 0, \text{if}\ i \in \mathcal{I}^-
			      \end{aligned}$$

		      所以
		      $$\begin{aligned}
				      \lim_{\lambda\to +\infty} \log\left( \cfrac{1}{1+\exp(\langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle)} \right)                                                               & = -\infty, \text{if}\ i \in \mathcal{I}^+\ \text{and}\ \langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle <0 \\
				      \lim_{\lambda\to +\infty} \log\left( \cfrac{\exp(\langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle)}{1+\exp(\langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle)} \right) & = -\infty, \text{if}\ i \in \mathcal{I}^-\ \text{and}\ \langle \lambda\mathbf{w},\overline{\mathbf{x}}_i \rangle <0
			      \end{aligned}$$

		      所以在线性不可分的情况下，若 $\|\mathbf{w}\|\to+\infty$, 则
		      $$L(\mathbf{w})\to+\infty$$
		      而 $L(\mathbf{0})$ 有限，因此 $L(\mathbf{w})$ 存在最小值。

		      在 $\rank{\overline{\mathbf{X}}}=d+1$ 的条件下，可知最小值是唯一的。

	\end{enumerate}
\end{solution}


\newpage
\begin{exercise}[Programming Exercise: Naive Bayes  \textnormal{30pts}]
	We provide you with a data set that contains spam and non-spam emails (``hw3\_nb.zip"). Please use the Naive Bayes Classifier to detect the spam emails.
	Finish the following exercises by programming. You can use your favorite programming language.
	\begin{enumerate}
		\item Remove all the tokens that contain non-alphabetic characters.
		\item Train the Naive Bayes Classifier on the training set according to Algorithm \ref{alg:train_bayes}.
		\item Test the Naive Bayes Classifier on the test set according to Algorithm \ref{alg:test_bayes}.
		\item Compute the confusion matrix, precision, recall and F1 score and then write down them in this file.
	\end{enumerate}

\end{exercise}

\begin{algorithm}
	\caption{Training Naive Bayes Classifier}
	\label{alg:train_bayes}
	\textbf{Input:} The training set with the labels $\mathcal{D}=\{(\textbf{x}_i,y_i)\}.$
	\begin{algorithmic}[1]
		\STATE $\mathcal{V}\leftarrow$ the set of distinct words and other tokens found in $\mathcal{D}$\\
		\FOR{each target value $c$ in the lables set $\mathcal{C}$}
		\STATE $\mathcal{V}_c\leftarrow$ the training samples whose labels are $c$\\
		\STATE $P(c)\leftarrow\frac{|\mathcal{V}_c|}{|\mathcal{V}|}$\\
		\STATE $T_c\leftarrow$ a single document by concaatenating all training samples in $\mathcal{V}_c$\\
		\STATE $n_c\leftarrow |T_c|$
		\FOR{each word $w_k$ in the vocabulary $\mathcal{V}$}
		\STATE $n_{c,k}\leftarrow$ the number of times the word $w_k$ occurs in $T_c$\\
		\STATE $P(w_k|c)=\frac{n_{c,k}+1}{n_c+|\mathcal{V}|}$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Testing Naive Bayes Classifier }
	\label{alg:test_bayes}
	\textbf{Input:} An email $\textbf{x}$. Let $x_i$ be the $i^{th}$ token in $\textbf{x}$ . $\mathcal{I}=\emptyset.$
	\begin{algorithmic}[1]
		\FOR{$i=1,\dots,|\textbf{x}|$}
		\IF{$\exists\, w_{ki}\in\mathcal{V}$ such that $w_{ki}=x_i$}
		\STATE $\mathcal{I}\leftarrow\mathcal{I}\cup k_i$
		\ENDIF
		\ENDFOR
		\STATE predict the label of $\textbf{x}$ by
		\begin{align*}
			\hat{y}=\arg\max_{c\in\mathcal{C}} P(c)\prod_{i\in\mathcal{I}}P(w_{ki}|c)
		\end{align*}
	\end{algorithmic}
\end{algorithm}

\begin{solution}
	设垃圾邮件为正例，非垃圾邮件为反例。
	$$\begin{tabular}{|c|c|c|}
			\hline
			\diagbox{Actual}{Predicted} & Spam & Non-spam \\
			\hline
			Spam                        & 47   & 2        \\
			\hline
			Non-spam                    & 0    & 242      \\
			\hline
		\end{tabular}$$

	$$\begin{aligned}
			\text{Precision} & =1.0                \\
			\text{Recall}    & =0.9591836734693877 \\
			\text{F1 score}  & =0.9791666666666665
		\end{aligned}$$

\end{solution}

\newpage
\begin{exercise}[Programming Exercise: Logistic Regression \textnormal{30pts}]
	We provide you with a data set that contains images fall into two classes (``hw3\_lr.zip''). Please use the Logistic Regression to classify them.
	Finish the following exercises by programming. You can use your favorite programming language.
	\begin{enumerate}
		\item Choose a proper normalization method to process the data matrix.
		\item Train the Logistic Regression Classifier on the training set.
		\item Test the Logistic Regression Classifier on the test set. Compute the confusion matrix, precision, recall and F1 score and then write down them in this file.
	\end{enumerate}
\end{exercise}

\begin{solution}
	设标识为 2 的图片为正例，标识为 1 的图片为反例。
	$$\begin{tabular}{|c|c|c|}
			\hline
			\diagbox{Actual}{Predicted} & 2    & 1    \\
			\hline
			2                           & 1025 & 7    \\
			\hline
			1                           & 7    & 1128 \\
			\hline
		\end{tabular}$$

	$$\begin{aligned}
			\text{Precision} & =0.9932170542635659 \\
			\text{Recall}    & =0.9932170542635659 \\
			\text{F1 score}  & =0.9932170542635659
		\end{aligned}$$

\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
