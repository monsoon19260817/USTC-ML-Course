%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages} 
\usepackage{enumitem}
\usepackage{color}
\usepackage{ctex}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\pagestyle{fancy}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}
\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{xeCJK}

% \setmainfont{MicrosoftYaHei}
\setCJKmainfont{MicrosoftYaHei}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\figref}[1]{Fig.~\ref{#1}}

%% Homework info.
\newcommand{\posted}{\text{Mar. 5, 2020}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Mar. 12, 2020}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{1}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Jiahuan Yu}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB17121687}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\dom}{ \textbf{dom}  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\argmax}{\bf argmax}

\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
    {\bf\large Introduction to Machine Learning}\\
    {Spring 2020}\\
    University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name
\hfill
ID: \id
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please present your solutions step by step.



% (3)(5)
\begin{exercise}[Rank of matrices \textnormal{25pts}]
    Let $\mathbf{A} \in \mathbb{R}^{m\times n}$ and $\mathbf{B}\in \mathbb{R}^{n\times p}$.
    \begin{enumerate}
        \item Please show that
              \begin{enumerate}
                  \item $\rank{\mathbf{A}} = \rank{\mathbf{A}^{\top}}$;
                  \item $\rank{\mathbf{A}\mathbf{B}} \leq \rank{\mathbf{A}}$;
                  \item $\rank{\mathbf{A}\mathbf{B}} \leq \rank{\mathbf{B}}$;
                  \item $\rank{\mathbf{A}} = \rank{\mathbf{A}^{\top}  \mathbf{A}}$.
              \end{enumerate}
        \item The \emph{column space} of $\mathbf{A}$ is defined by
              \begin{align*}
                  \mathcal{C}(\mathbf{A} ) = \{ \mathbf{y}\in \mathbb{R}^m : \mathbf{y} = \mathbf{Ax},\,\mathbf{x}\in\mathbb{R}^n\}.
              \end{align*}
              The \emph{null space} of $\mathbf{A}$ is defined by
              \begin{align*}
                  \mathcal{N}(\mathbf{A})  = \{ \mathbf{x}\in \mathbb{R}^n : \mathbf{Ax}=0\}.
              \end{align*}
              Notice that, the rank of $\mathbf{A}$ is the dimension of the column space of $\mathbf{A}$.

              Please show that:
              \begin{enumerate}
                  \item $\rank{\mathbf{A}} + \dim ( \mathcal{N}( \mathbf{A} ) ) = n$;
                  \item $\mathbf{y}=\mathbf{0}$ if and only if $\mathbf{a}_i^{\top}\mathbf{y}=0$ for $i=1,\ldots,m$, where $\mathbf{y}\in \mathbb{R}^m$ and  $\{\mathbf{a}_1,\mathbf{a}_2,\ldots,\mathbf{a}_m\}$ is a basis of $\mathbb{R}^m$.
              \end{enumerate}
        \item Show that
              \begin{align}\label{eqn:rankaba}
                  \rank{\mathbf{AB}}=\rank{\mathbf{B}}-\dim(\mathcal{C}(\mathbf{B})\cap \mathcal{N}(\mathbf{A})).
              \end{align}
        \item Suppose that the first term on the right-hand side (RHS) of \eqref{eqn:rankaba} changes to $\rank{\mathbf{A}}$. Please find the second term on the RHS of \eqref{eqn:rankaba} such that it still holds.
        \item Show the results in 1. by \eqref{eqn:rankaba} or the one you established in 4.
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item \begin{enumerate}
                  \item 设 $\rank{\mathbf{A}}=r$, 由秩的定义可知
                        $$\mathbf{A}=\mathbf{P}\begin{pmatrix}
                                \mathbf{I}_r & \mathbf{O} \\
                                \mathbf{O}   & \mathbf{O}
                            \end{pmatrix}\mathbf{Q}$$
                        其中 $\mathbf{P}\in\mathbb{R}^{m\times m}$, $\mathbf{Q}\in\mathbb{R}^{n\times n}$, 两者都是满秩矩阵。
                        $$\mathbf{A}^\top=\mathbf{Q}^\top\begin{pmatrix}
                                \mathbf{I}_r & \mathbf{O} \\
                                \mathbf{O}   & \mathbf{O}
                            \end{pmatrix}\mathbf{P}^\top$$
                        所以 $\rank{\mathbf{A}^\top}=r=\rank{\mathbf{A}}$.
                  \item $\rank{A}=\dim(\mathcal{C}( \mathbf{A} ))$, $\rank{AB}=\dim(\mathcal{C}( \mathbf{AB} ))$.

                        设 $\mathbf{B}=(\mathbf{b}_1,\mathbf{b}_2,\cdots,\mathbf{p}_p)$.

                        有 $\mathbf{A}\mathbf{b}_i\in\mathcal{C}(\mathbf{A})$.

                        所以 $\mathcal{C}(\mathbf{AB})\subseteq \mathcal{C}(A)$.

                        所以 $\rank{\mathbf{AB}}\leq\rank{\mathbf{A}}$.
                  \item $$\rank{\mathbf{AB}}=\rank{\mathbf{B}^\top\mathbf{A}^\top}\leq \rank{\mathbf{B}^\top}=\rank{\mathbf{B}}$$
                  \item 设 $\mathbf{A}=(\mathbf{a}_1,\mathbf{a}_2,\cdots,\mathbf{n})$.

                        所以 $\mathbf{A}^\top\mathbf{a}_i=\mathbf{A}^\top(\mathbf{A}\mathbf{e}_i)=\mathbf{A}^\top\mathbf{A}\mathbf{e}_i$, 其中 $\mathbf{e}_i =(0,0,\cdots,1,\cdots,0,0)^\top \in \mathbb{R}^n$, 只有第 $i$ 个元素为 $1$.

                        所以 $\mathbf{A}^\top\mathbf{a}_i \in \mathcal{C}(\mathbf{A}^\top \mathbf{A})$, 所以 $\rank{\mathbf{A}^\top}=\rank{\mathbf{A}}\leq \rank{\mathbf{A}^\top\mathbf{A}}$.

                        又因为 $\rank{\mathbf{A}^\top\mathbf{A}}\leq \rank{\mathbf{A}}$, 所以 $\rank{\mathbf{A}^\top\mathbf{A}}=\rank{\mathbf{A}}$.
              \end{enumerate}
        \item \begin{enumerate}
                  \item 因为 $\rank{\mathbf{A}}=r$, 所以 $\mathbf{A}$ 可以通过初等行变换变为以下形式
                        $$\mathbf{J}=\begin{pmatrix}
                                c_{11} & \cdots & c_{1,j_2 -1} & \cdots    & \cdots & \cdots       & \cdots    & \cdots & c_{1n} \\
                                0      & \cdots & 0            & c_{2,j_2} & \cdots & c_{2,j_3 -1} & \cdots    & \cdots & c_{2n} \\
                                0      & \cdots & \cdots       & 0         & \cdots & 0            & c_{3,j_3} & \cdots & c_{3n} \\
                                \vdots &        &              & \vdots    &        & \vdots       & \vdots    & \ddots & \vdots \\
                                0      & \cdots & \cdots       & 0         & \cdots & 0            & 0         & \cdots & c_{rn} \\
                                0      & \cdots & \cdots       & \cdots    & \cdots & \cdots       & \cdots    & \cdots & 0      \\
                                \vdots &        &              &           &        &              &           &        & \vdots \\
                                0      & \cdots & \cdots       & \cdots    & \cdots & \cdots       & \cdots    & \cdots & 0
                            \end{pmatrix}$$
                        所以 $\mathbf{Ax}=0$ 等价于 $\mathbf{Jx}=0$.

                        记 $\mathbf{x}$ 中除了 $x_{j_1}, x_{j_2}, \cdots, x_{j_r} (j_1=1)$ 以外的分量为 $t_1, t_2, \cdots, t_{n-r}$

                        当 $t_1, t_2, \cdots, t_{n-r}$ 确定后，剩下的分量组成线性方程组
                        $$\begin{pmatrix}
                                c_{1 j_1} & c_{1 j_2} & \cdots & c_{1 j_r} \\
                                0         & c_{2 j_2} & \cdots & c_{2 j_r} \\
                                \vdots    & \vdots    & \ddots & \vdots    \\
                                0         & 0         & \cdots & c_{r j_r}
                            \end{pmatrix}
                            \begin{pmatrix}
                                x_{j_1} \\ x_{j_2}\\ \vdots\\ x_{j_r}
                            \end{pmatrix}=0$$
                        可继续化为
                        $$\begin{pmatrix}
                                d_{11} & 0      & \cdots & 0      \\
                                0      & d_{22} & \cdots & 0      \\
                                \vdots & \vdots & \ddots & \vdots \\
                                0      & 0      & \cdots & d_{rr}
                            \end{pmatrix}
                            \begin{pmatrix}
                                x_{j_1} \\ x_{j_2}\\ \vdots\\ x_{j_r}
                            \end{pmatrix}=0$$
                        此方程组有唯一解 $x_{j_1}=x_{j_2}=\cdots=x_{j_r}=0$.

                        因此 $\mathbf{Ax}=0$ 的解可以写成

                        $$\begin{pmatrix}
                                x_1 \\ x_2 \\ \vdots \\ x_n
                            \end{pmatrix}
                            =\begin{pmatrix}
                                h_{11} & h_{12} & \cdots & h_{1,n-r} \\
                                h_{21} & h_{22} & \cdots & h_{2,n-r} \\
                                \vdots & \vdots & \ddots & \vdots    \\
                                h_{n1} & h_{n2} & \cdots & h_{n,n-r}
                            \end{pmatrix}
                            \begin{pmatrix}
                                t_1 \\t_2 \\\vdots\\t_{n-r}
                            \end{pmatrix}
                            =t_1\mathbf{h_1}+t_2\mathbf{h_2}+\cdots+t_{n-r}\mathbf{h_{n-r}}$$
                        其中
                        $$\mathbf{h_i}=\begin{pmatrix}
                                h_{1i} \\h_{2i}\\\vdots\\h_{ni}
                            \end{pmatrix}$$

                        若 $t_1\mathbf{h_1}+t_2\mathbf{h_2}+\cdots+t_{n-r}\mathbf{h_{n-r}}=0$.

                        则 $x_1=x_2=\cdots=x_n=0$.

                        则 $t_1=t_2=\cdots=t_{n-r}=0$.

                        所以 $h_1, h_2, \cdots, h_{n-r}$ 相互独立。

                        所以 $\dim(\mathcal{N}(\mathbf{A})=n-r$.

                        所以 $\rank{\mathbf{A}}+\dim(\mathcal{N}(\mathbf{A}))=n$.
                  \item \begin{enumerate}
                            \item 必要性：$\mathbf{y}=0 \implies \mathbf{a}_i^\top \mathbf{y}=0$.

                                  显然。

                            \item 充分性：$\mathbf{y}=0 \impliedby \mathbf{a}_i^\top \mathbf{y}=0$.

                                  $$(\mathbf{a}_1+\mathbf{a}_2+\cdots+\mathbf{a}_m)\mathbf{y}=0$$

                                  因为 $\left\{\mathbf{a}_1, \mathbf{a}_2,\cdots,\mathbf{a}_m\right\}$ 是基向量，所以线性无关.

                                  所以 $\mathbf{y}=0$.
                        \end{enumerate}
              \end{enumerate}
        \item 显然 $\mathcal{C}(\mathbf{AB})=\mathcal{C}(\mathbf{A})\cap\mathcal{C}(\mathbf{B})$

              $\mathbb{R}^n$ 空间有 4 个子空间: $\mathcal{C}(\mathbf{A})\cap\mathcal{C}(\mathbf{B})$, $\mathcal{C}(\mathbf{A})\cap\mathcal{N}(\mathbf{B})$, $\mathcal{N}(\mathbf{A})\cap\mathcal{C}(\mathbf{B})$, $\mathcal{N}(\mathbf{A})\cap\mathcal{N}(\mathbf{B})$

              任意两个子空间取 $\cap$ 都得到空集. 与此同时

              $\rank{\mathbf{AB}}=\dim(\mathcal{C}(\mathbf{AB}))=\dim(\mathcal{C}(\mathbf{A})\cap\mathcal{C}(\mathbf{B}))$

              $\rank{B} = \dim(\mathcal{C}(\mathbf{A})\cap\mathcal{C}(\mathbf{B}))+\dim(\mathcal{N}(\mathbf{A})\cap\mathcal{C}(\mathbf{B}))$

              即可得证.
        \item 与上一小题类似,

              $\rank{A} = \dim(\mathcal{C}(\mathbf{A})\cap\mathcal{C}(\mathbf{B}))+\dim(\mathcal{C}(\mathbf{A})\cap\mathcal{N}(\mathbf{B}))$

              所以 $$\rank{\mathbf{A}\mathbf{B}}=\rank{\mathbf{A}}-\dim(\mathcal{C}(\mathbf{A}) \cap \mathcal{N}(\mathbf{B}))$$
        \item \begin{enumerate}
                  \item 
                  \item 由 \eqref{eqn:rankaba},
                        $$\rank{\mathbf{B}^\top\mathbf{A}^\top}\leq\rank{\mathbf{A}^\top}$$
                        左右都取转置，得
                        $$\rank{\mathbf{A}\mathbf{B}}\leq\rank{\mathbf{A}}$$
                  \item 由 \eqref{eqn:rankaba}, 显然。
                  \item $$\rank{\mathbf{A}^\top\mathbf{A}}=\rank{\mathbf{A}}-\dim(\mathcal{C}(\mathbf{A}) \cap \mathcal{N}(\mathbf{A}^\top))$$
                  $$\rank{\mathbf{A}^\top\mathbf{A}}=\rank{\mathbf{A}^\top}-\dim(\mathcal{C}(\mathbf{A}^\top) \cap \mathcal{N}(\mathbf{A}))$$

              \end{enumerate}
    \end{enumerate}
\end{solution}
\newpage



% done
\begin{exercise}[Linear equations
        \textnormal{15pts}]

    Consider the system of linear equations in $\mathbf{w}$
    \begin{align}\label{eq1}
        \mathbf{y} = \mathbf{X} \mathbf{w} ,
    \end{align}
    where $\mathbf{y} \in \mathbb{R}^{n}$, $\mathbf{w} \in \mathbb{R}^{d}$, and $\mathbf{X} \in \mathbb{R}^{n \times d}$.

    \begin{enumerate}
        \item Give an example for ``$\mathbf{X}$'' and ``$\mathbf{y}$'' to satisfy the following three situations respectively:
              \begin{enumerate}
                  \item there exists one unique solution;
                  \item there does not exist any solution;
                  \item there exists more than one solution.
              \end{enumerate}

        \item Suppose that $\mathbf{X}$ has full column rank and $\rank{(\mathbf{X}, \mathbf{y})} = \rank{\mathbf{X}}$. Show that the system of linear equations (\ref{eq1}) always admits a unique solution.

        \item (\textbf{Normal equations}) Consider  another system of linear equations in $\mathbf{w}$
              \begin{align}\label{eq_normal}
                  \mathbf{X}^{\top}\mathbf{y} = \mathbf{X}^{\top}\mathbf{X}\mathbf{w}.
              \end{align}
              Please show that the system (\ref{eq_normal}) always admits a solution. Moreover, does it always admit a unique solution?
    \end{enumerate}

\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item \begin{enumerate}
                  \item $\mathbf{X}=\begin{pmatrix}
                                1 & 0 \\ 0 & 1
                            \end{pmatrix}$, $\mathbf{y}=\begin{pmatrix}
                                0 \\ 0
                            \end{pmatrix}$
                  \item $\mathbf{X}=\begin{pmatrix}
                                1 & 0 \\ 1 & 0
                            \end{pmatrix}$, $\mathbf{y}=\begin{pmatrix}
                                1 \\ 0
                            \end{pmatrix}$
                  \item $\mathbf{X}=\begin{pmatrix}
                                1 & 0 \\ 1 & 0
                            \end{pmatrix}$, $\mathbf{y}=\begin{pmatrix}
                                0 \\ 0
                            \end{pmatrix}$
              \end{enumerate}
        \item 因为 $\rank{(\mathbf{X}, \mathbf{y})} = \rank{\mathbf{X}}$, 所以 $\mathbf{y}$ 可用 $\mathbf{X}$ 的列向量线性表示，所以 $\mathbf{y}=\mathbf{X}\mathbf{w}$ 有解。

              设 $\mathbf{w}_1$ 和 $\mathbf{w}_2$ 是两个解，则
              $$\mathbf{X}(\mathbf{w}_1-\mathbf{w}_2)=\mathbf{0}$$
              因为 $\mathbf{X}$ 列满秩，所以 $\mathbf{w}_1-\mathbf{w}_2=\mathbf{0}$, 所以有唯一解。
        \item
              设 $$f(\mathbf{w})=\|\mathbf{X}\mathbf{w}-\mathbf{y}\|_2^2$$
              所以
              $$\nabla_\mathbf{w} f(\mathbf{w})=\mathbf{X}^\top\mathbf{X}\mathbf{w}-\mathbf{X}^\top\mathbf{w}$$
              $f(\mathbf{w})$ 作为范数函数一定有极小值, 所以一定存在 $\mathbf{w}$ 使得 $\nabla_\mathbf{w} f(\mathbf{w})=0$.

              所以 $\mathbf{X}^\top\mathbf{y}=\mathbf{X}^\top\mathbf{X}\mathbf{w}$ 必定有解.

              不一定有唯一解，例如 $\mathbf{X}=\begin{pmatrix}
                      0 & 0 \\ 0 & 0
                  \end{pmatrix}$, $\mathbf{y}=\begin{pmatrix}
                      0 \\ 0
                  \end{pmatrix}$ 有无穷解。
    \end{enumerate}
\end{solution}

\newpage


% done
\begin{exercise}[Basis and coordinates \textnormal{15pts}]

    Suppose that $\{\mathbf{a}_1, \mathbf{a}_2,\dots,\mathbf{a}_n\}$ is a basis of an $n$-dimensional vector space $V$.
    \begin{enumerate}
        \item Show that $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$ is also a basis of $V$ for any nonzero scalars $\lambda_1,\lambda_2, \dots, \lambda_n$.
        \item Let $V =\mathbb{R}^n$ and  $(\mathbf{b}_1,\mathbf{b}_2,\dots, \mathbf{b}_n) = (\mathbf{a}_1,\mathbf{a}_2, \dots, \mathbf{a}_n)\mathbf{P}$, where $\mathbf{P}\in \mathbb{R}^{n\times n}$ and $\mathbf{b}_i\in \mathbb{R}^n$, for any $i\in\{1,\dots,n\}$. Show that $\{ \mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n\}$ is also a basis of $V$ for any invertible  matrix $\mathbf{P}$.
        \item Suppose that the coordinate of a vector $\mathbf{v}$ under the basis $\{\mathbf{a}_1,  \mathbf{a}_2,\dots,\mathbf{a}_n\}$ is $\mathbf{x}=(x_1,x_2,\dots x_n)$.
              \begin{enumerate}
                  \item What is the coordinate of $\mathbf{v}$ under $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$?

                  \item What are the coordinates of $\mathbf{w} = \mathbf{a}_1+\dots + \mathbf{a}_n$ under $\{\mathbf{a}_1, \mathbf{a}_2,\dots,\mathbf{a}_n\}$ and $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$? Note that  $\lambda_i \neq 0$ for any $i\in \{1,\dots,n\}$.
              \end{enumerate}
    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item $\forall \mathbf{v}\in \mathbf{V}$
              $$\mathbf{v}=\sum_{i=1}^n t_i \mathbf{a}_i=\sum_{i=1}^n \cfrac{t_i}{\lambda_i} \lambda_i (\lambda_i \mathbf{a}_i)$$
              同时 $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$ 的任意线性组合
              $$\mathbf{v}'=\sum_{i=1}^n s_i (\lambda_i \mathbf{a}_i)=\sum_{i=1}^n (s_i \lambda_i) \mathbf{a}_i\in \mathbf{V}$$
              所以  $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$ 也是一组基。
        \item 设 $\mathbf{t}=(t_1,\cdots,t_n)^\top$, $\mathbf{s}=(s_1,\cdots,s_n)^\top$. $\forall \mathbf{v}\in\mathbf{V}$
              $$\mathbf{v}=(\mathbf{a}_1,\mathbf{a}_2, \dots, \mathbf{a}_n)\mathbf{t}=(\mathbf{b}_1,\mathbf{b}_2,\dots, \mathbf{b}_n)\mathbf{P}^{-1}\mathbf{t}=(\mathbf{b}_1,\mathbf{b}_2,\dots, \mathbf{b}_n)\mathbf{s}$$
              同时 $\{\lambda_1 \mathbf{a}_1, \lambda_2 \mathbf{a}_2, \dots, \lambda_n\mathbf{a}_n\}$ 的任意线性组合
              $$\mathbf{v}'=(\mathbf{b}_1,\mathbf{b}_2,\dots, \mathbf{b}_n)\mathbf{s}=(\mathbf{b}_1,\mathbf{b}_2,\dots, \mathbf{b}_n)\mathbf{P}^{-1}\mathbf{t}=(\mathbf{a}_1,\mathbf{a}_2, \dots, \mathbf{a}_n)\mathbf{t}\in \mathbf{V}$$
              所以 $\{ \mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_n\}$ 也是一组基。
        \item \begin{enumerate}
                  \item $(\cfrac{x_1}{\lambda_1},\cfrac{x_2}{\lambda_2},\cdots,\cfrac{x_n}{\lambda_n})$.
                  \item 分别为 $(1,1,\cdots,1)$ 和 $(\cfrac{1}{\lambda_1},\cfrac{1}{\lambda_2},\cdots,\cfrac{1}{\lambda_n})$.
              \end{enumerate}
    \end{enumerate}
\end{solution}

\newpage


% done
\begin{exercise}[ \textnormal{5pts}]
    Let $\mathbf{x},\mathbf{a}\in \mathbb{R}^n$ and $\mathbf{y}\in \mathbb{R}^m$. Find the gradients of the following functions.
    \begin{enumerate}
        \item $f(\mathbf{x}) = \mathbf{a}^{\top}\mathbf{x}$.
        \item $f(\mathbf{x}) = \mathbf{x}^{\top}\mathbf{x}$.
        \item $f(\mathbf{x})=\| \mathbf{y} - \mathbf{A}\mathbf{x} \|_2^2$, where $\mathbf{A}\in\mathbb{R}^{m\times n}$.
        \item  $f(\mathbf{X}) = \det(\mathbf{X})$, where $\det(\mathbf{X})$ is the determinant of $\mathbf{X} \in \mathbb{R}^{n \times n}$.
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item $$\nabla f(\mathbf{x}) = \mathbf{a}$$
        \item $$\nabla f(\mathbf{x}) = 2\mathbf{x}$$
        \item $$\begin{aligned}
                      \nabla f(\mathbf{x})
                       & = \nabla \left( (\mathbf{y}-\mathbf{A}\mathbf{x})^\top(\mathbf{y}-\mathbf{A}\mathbf{x}) \right)      \\
                       & = 2\nabla (\mathbf{y}-\mathbf{A}\mathbf{x}) \cdot (\mathbf{y}-\mathbf{A}\mathbf{x})                  \\
                       & = 2\left( \nabla \mathbf{y}- \nabla (\mathbf{A}\mathbf{x}) \right) (\mathbf{y}-\mathbf{A}\mathbf{x}) \\
                       & = -2 \mathbf{A}^\top (\mathbf{y}-\mathbf{A}\mathbf{x})
                  \end{aligned}$$
        \item $$\cfrac{\partial \det(\mathbf{X})}{\partial x_{ij}}=X_{ij}$$
              其中 $X_{ij}$ 为 $\mathbf{X}$ 关于 $x_{ij}$ 的代数余子式。所以
              $$\nabla \det(\mathbf{X})=\sum_i \sum_j X_{ij}$$
    \end{enumerate}
\end{solution}


\newpage

% done
\begin{exercise}[Linear regression \textnormal{20pts}]
    Consider a data set $\{ (x_i ,y_i) \}_{i=1}^{n}$, where $x_i,y_i\in \mathbb{R}$.
    \begin{enumerate}
        \item If we want to fit the data by a linear model
              \begin{align}\label{eqn:linear}
                  y =  w_0 + w_1 x,
              \end{align}
              please find $w_0$ and $w_1$ by the least squares approach (you need to find expressions of $w_0$ and $w_1$ by $\{ (x_i ,y_i) \}_{i=1}^{n}$, respectively).
        \item \textbf{Programming Exercise} We provide you a data set $\{ (x_i ,y_i) \}_{i=1}^{30}$. Consider the model in (\ref{eqn:linear}) and the one as follows:
              \begin{align}\label{eqn:linear-quadratic}
                  y =  w_0 + w_1 x+ w_2 x^2.
              \end{align}
              Which model do you think fits better the data? Please detail your approach first and then implement it by your favorite programming language. The required output includes
              \begin{enumerate}
                  \item your detailed approach step by step;
                  \item your code with detailed comments according to your planned approach;
                  \item a plot showing the data and the fitting models;
                  \item the model you finally choose [$w_0$ and $w_1$ if you choose the model in (\ref{eqn:linear}), or $w_0$, $w_1$, and $w_2$ if you choose the model in (\ref{eqn:linear-quadratic})].
              \end{enumerate}
    \end{enumerate}

\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item 定义
              $$S(f(i))=\sum_{i=1}^{n}{f(i)}$$
              其中 $f(i)$ 为任意关于 $i$ 的表达式。令
              $$ L = S^2(w_0+w_1 x_i-y_i)$$
              要求
              $$\argmin_{w_0,w_1}  L$$
              则
              $$\begin{aligned}
                      \frac{\partial L}{\partial w_0}
                       & = 2 S(w_0+w_1 x_i-y_i)           \\
                       & = 2nw_0 + 2w_1 S(x_i) - 2 S(y_i)
                  \end{aligned}$$
              $$\begin{aligned}
                      \frac{\partial L}{\partial w_1}
                       & = 2 S((w_0+w_1 x_i-y_i)x_i)                   \\
                       & = 2w_0 S(x_i) + 2 w_1 S(x_i^2) - 2 S(x_i y_i)
                  \end{aligned}$$
              令
              $$\frac{\partial L}{\partial w_0} = \frac{\partial L}{\partial w_1} = 0$$
              解得
              $$\begin{aligned}
                      \hat{w}_0 & = \frac
                      {S({x_i}^2) S(y_i) - S(x_i) S(x_i y_i)}
                      {n S({x_i}^2) - S^2(x_i)} \\
                      \hat{w}_1 & = \frac
                      {n S(x_i y_i) - S(x_i) S(y_i)}
                      {n S({x_i}^2) - S^2(x_i)}
                  \end{aligned}$$
        \item $$L=S^2(w_0+w_1 x_i +w_2 x_i^2 -y_i)$$
              令
              $$\frac{\partial L}{\partial w_0}
                  =\frac{\partial L}{\partial w_1}
                  =\frac{\partial L}{\partial w_2}=0$$
              得
              $$\begin{bmatrix}
                      n        & S(x_i)   & S(x_i^2) \\
                      S(x_i)   & S(x_i^2) & S(x_i^3) \\
                      S(x_i^2) & S(x_i^3) & S(x_i^4)
                  \end{bmatrix}
                  \begin{bmatrix}
                      w_0 \\  w_1 \\  w_2
                  \end{bmatrix}
                  =\begin{bmatrix}
                      S(y_i) \\ S(x_i y_i) \\ S(x_i^2 y_i)
                  \end{bmatrix}$$
              求解的结果图片见 plot1.2.png, 代码见 solution1.2.py

              线性回归 $L=0.28216799684393734$,

              二次回归 $L=0.24249332353528988$,

              最终选择二次回归，

              $$\begin{cases}
                      w_0=1.029568 \\
                      w_1=0.386143 \\
                      w_2=-0.142151
                  \end{cases}$$

    \end{enumerate}
\end{solution}
\newpage


% 证明对称矩阵
\begin{exercise}[Projection \textnormal{30pts}]
    Let $\mathbf{A}\in\mathbb{R}^{m\times n}$ and $\mathbf{x} \in \mathbb{R}^m$. Define
    \begin{align*}
        \proj{\mathbf{x}}{\mathbf{A}} = \argmin_{\mathbf{z}\in\mathbb{R}^m}\,\{\|\mathbf{x}-\mathbf{z}\|_2: \mathbf{z}\in\mathcal{C}(\mathbf{A})\}.
    \end{align*}
    We call $\proj{\mathbf{x}}{\mathbf{A}}$ the projection of the point $\mathbf{x}$ onto the column space of $\mathbf{A}$.
    \begin{enumerate}
        \item Please prove that $\mathbf{P}_{\mathbf{A}}(\mathbf{x})$ is unique for any $\mathbf{x} \in \mathbb{R}^m$.
        \item Let $\mathbf{v}_i \in \mathbb{R}^n$, $i=1,\ldots,d$ with $d\leq n$, which are linearly independent.
              \begin{enumerate}
                  \item For any $\mathbf{w}\in \mathbb{R}^n$, please find $\proj{\mathbf{w}}{\mathbf{v}_1}$, which is the projection of $\mathbf{w}$ onto the subspace spanned by $\mathbf{v}_1$.
                  \item Please show $\proj{\cdot}{\mathbf{v}_1}$ is a linear map, i.e.,
                        \begin{align*}
                            \proj{\alpha\mathbf{u}+\beta\mathbf{w}}{\mathbf{v}_1}=\alpha\proj{\mathbf{u}}{\mathbf{v}_1} + \beta \proj{\mathbf{w}}{\mathbf{v}_1},
                        \end{align*}
                        where $\alpha,\beta\in\mathbb{R}$ and $\mathbf{w}\in\mathbb{R}^n$.
                  \item Please find the projection matrix corresponding to the linear map $\proj{\cdot}{\mathbf{v}_1}$, i.e., find the matrix $\mathbf{H}_1\in\mathbb{R}^{n\times n}$ such that
                        \begin{align*}
                            \proj{\mathbf{w}}{\mathbf{v}_1}=\mathbf{H}_1\mathbf{w}.
                        \end{align*}
                  \item Let $\mathbf{V}=(\mathbf{v}_1,\ldots,\mathbf{v}_d)$.
                        \begin{enumerate}
                            \item For any $\mathbf{w}\in \mathbb{R}^n$, please find $\proj{\mathbf{w}}{\mathbf{V}}$ and the corresponding projection matrix $\mathbf{H}$.
                            \item Please find $\mathbf{H}$ if we further assume that $\mathbf{v}_i^{\top}\mathbf{v}_j=0$, $\forall\,i\neq j$.
                        \end{enumerate}
              \end{enumerate}

        \item
              \begin{enumerate}
                  \item Suppose that
                        \begin{align*}
                            \mathbf{A} = \left[
                                \begin{matrix}
                                    1 & 0 \\
                                    0 & 1
                                \end{matrix}
                                \right] .
                        \end{align*}
                        What are the coordinates of $\mathbf{P}_{\mathbf{A}}(\mathbf{x})$ with respect to the column vectors in $\mathbf{A}$ for any $\mathbf{x} \in \mathbb{R}^m$? Are the coordinates unique?
                  \item Suppose that
                        \begin{align*}
                            \mathbf{A} = \left[
                                \begin{matrix}
                                    1 & 2 \\
                                    1 & 2
                                \end{matrix}
                                \right] .
                        \end{align*}
                        What are the coordinates of $\mathbf{P}_{\mathbf{A}}(\mathbf{x})$ with respect to the column vectors in $\mathbf{A}$ for any $\mathbf{x} \in \mathbb{R}^m$? Are the coordinates unique?
              \end{enumerate}

        \item A matrix $\mathbf{P}$ is called a projection matrix if $\mathbf{P}\mathbf{x}$ is the projection of $\mathbf{x}$ onto $\mathcal{C}(\mathbf{P})$ for any $\mathbf{x}$.
              \begin{enumerate}
                  \item Let $\lambda$ be the eigenvalue of $\mathbf{P}$. Show that $\lambda$ is either $1$ or $0$. (\emph{Hint: you may want to figure out what the eigenspaces corresponding to $\lambda=1$ and $\lambda=0$ are, respectively.})
                  \item Show that $\mathbf{P}$ is a projection matrix if and only if $\mathbf{P}^2 = \mathbf{P}$ and $\mathbf{P}$ is symmetric.
              \end{enumerate}


    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item 假设 $\mathbf{z}_1, \mathbf{z}_2$, $\mathbf{z}_1\neq\mathbf{z}_2$ 满足
              $$ \|\mathbf{x}-\mathbf{z}_1\|_2=\|\mathbf{x}-\mathbf{z}_2\|_2 = \min_{\mathbf{z}\in\mathbb{R}^m} \{\|\mathbf{x}-\mathbf{z}\|_2: \mathbf{z}\in\mathcal{C}(\mathbf{A})\}$$
              若 $\lambda\in[0,1]$, 则
              $$\begin{aligned}
                      \|\mathbf{x}-[\lambda \mathbf{z}_1+(1-\lambda)\mathbf{z}_2]\|_2
                       & =\|\lambda(\mathbf{x}-\mathbf{z}_1)+(1-\lambda)(\mathbf{x}-\mathbf{z}_2)\|_2                             \\
                       & \leq \lambda \|\mathbf{x}-\mathbf{z}_1\|_2+(1-\lambda)\|\mathbf{x}-\mathbf{z}_2\|_2                      \\
                       & = \min_{\mathbf{z}\in\mathbb{R}^m} \{\|\mathbf{x}-\mathbf{z}\|_2: \mathbf{z}\in\mathcal{C}(\mathbf{A})\}
                  \end{aligned}$$

              即线段 $\mathbf{z}_1\mathbf{z}_2$ 上任意一点到 $\mathbf{x}$ 的距离相等,这是不可能的.

              所以 $\proj{\mathbf{A}}{\mathbf{x}}$ 唯一.
        \item \begin{enumerate}
                  \item $\forall \mathbf{y}\in\lspan{\mathbf{v}_1}$, $\exists \lambda\in\mathbb{R}$, $\mathbf{y}=\lambda\mathbf{v}_1$. 所以
                        $$\begin{aligned}
                                \min_{\mathbf{y}\in\lspan{\mathbf{v}_1}}\|\mathbf{w}-\mathbf{y}\|_2^2
                                 & =\min_{\lambda\in\mathbb{R}}\|\mathbf{w}-\lambda\mathbf{v}_1\|_2^2                                                                             \\
                                 & =\min_{\lambda\in\mathbb{R}} \left( \lambda^2\|\mathbf{v}_1\|_2^2 -2\lambda \langle \mathbf{v}_1,\mathbf{w} \rangle+\|\mathbf{w}\|_2^2 \right) \\
                                 & =\min_{\lambda\in\mathbb{R}} \left(
                                \|\mathbf{v}_1\|_2^2\left( \lambda-\cfrac{\langle \mathbf{v}_1,\mathbf{w} \rangle}{\|\mathbf{v}_1\|_2^2} \right)^2- \cfrac{\langle \mathbf{v}_1,\mathbf{w} \rangle^2}{\|\mathbf{v}_1\|_2^2}+\|\mathbf{w}\|_2^2
                                \right)
                            \end{aligned}$$
                        所以 $$\proj{\mathbf{w}}{\mathbf{v}_1}=\cfrac{\langle \mathbf{v}_1,\mathbf{w} \rangle}{\|\mathbf{v}_1\|_2^2}\mathbf{v}_1$$
                  \item $$\begin{aligned}
                                \proj{\alpha\mathbf{u}+\beta\mathbf{w}}{\mathbf{v}_1}
                                 & =\cfrac{\langle \mathbf{v}_1,\alpha\mathbf{u}+\beta\mathbf{w} \rangle}{\|\mathbf{v}_1\|_2^2}\mathbf{v}_1                                                                             \\
                                 & = \alpha \cfrac{\langle \mathbf{v}_1,\mathbf{u} \rangle}{\|\mathbf{v}_1\|_2^2}\mathbf{v}_1 + \beta \cfrac{\langle \mathbf{v}_1,\mathbf{w} \rangle}{\|\mathbf{v}_1\|_2^2}\mathbf{v}_1 \\
                                 & = \alpha \proj{\mathbf{u}}{\mathbf{v}_1} + \beta \proj{\mathbf{w}}{\mathbf{v}_1}
                            \end{aligned}$$
                  \item $$\proj{\mathbf{w}}{\mathbf{v}_1}
                            =\cfrac{\langle \mathbf{v}_1,\mathbf{w} \rangle}{\|\mathbf{v}_1\|_2^2}\mathbf{v}_1
                            =\mathbf{v}_1\cfrac{\mathbf{v}_1^\top\mathbf{w}}{\mathbf{v}_1^\top\mathbf{v}_1}
                            =\cfrac{\mathbf{v}_1\mathbf{v}_1^\top}{\mathbf{v}_1^\top\mathbf{v}_1}\mathbf{w}$$
                        $$\mathbf{H}_1=\cfrac{\mathbf{v}_1\mathbf{v}_1^\top}{\mathbf{v}_1^\top\mathbf{v}_1}$$
                  \item \begin{enumerate}
                            \item 设 $\mathbf{z}\in\mathbf{V}$, $\mathbf{z}=\mathbf{V}\mathbf{\lambda}$, $\mathbf{\lambda}\in\mathbb{R}^{d\times 1}$.
                                  $$\begin{aligned}
                                          \|\mathbf{z}-\mathbf{w}\|_2^2
                                           & =\|\mathbf{V}\mathbf{\lambda}-\mathbf{w}\|_2^2 \\
                                           & =
                                          % todo
                                      \end{aligned}$$
                                  所以 $$\proj{\mathbf{w}}{\mathbf{V}}=\mathbf{V}\left(\mathbf{V}^\top\mathbf{V}\right)^{-1}\mathbf{V}^\top\mathbf{w}$$
                                  $$\mathbf{H}=\mathbf{V}\left(\mathbf{V}^\top\mathbf{V}\right)^{-1}\mathbf{V}$$
                            \item $$\mathbf{H}=\sum_{i=1}^d \cfrac{\mathbf{v}_i\mathbf{v}_i^\top}{\|\mathbf{v}_1\|_2^2}$$
                        \end{enumerate}
              \end{enumerate}
        \item \begin{enumerate}
                  \item $$\mathbf{H}=\begin{bmatrix}
                                1 & 0 \\0&1
                            \end{bmatrix}$$
                        所以 $\proj{\mathbf{x}}{\mathbf{A}}=\mathbf{x}$, 坐标即为 $\mathbf{x}$ 的坐标, 是唯一的.
                  \item 在 $\mathbf{A}$ 的柱空间中
                        $$\proj{\mathbf{x}}{\mathbf{A}}=\lambda\begin{bmatrix}
                                1 \\1
                            \end{bmatrix},\lambda\in\mathbb{R}$$
                        经计算得 $$\proj{\mathbf{x}}{\mathbf{A}}=\begin{bmatrix}
                                (x_1+x_2)/2 \\(x_1+x_2)/2
                            \end{bmatrix}$$
                        但在 $\mathbb{R}^m$ 空间中这个坐标不唯一, $3,4,\cdots,m$ 个分量可以任取值.
              \end{enumerate}
        \item \begin{enumerate}
                  \item 设 $\mathbf{x}$ 为特征值.
                        $$\mathbf{P}\mathbf{x}=\lambda\mathbf{x}$$
                        $$\mathbf{P}^2\mathbf{x}=\lambda\mathbf{P}\mathbf{x}=\lambda^2\mathbf{x}$$
                        所以 $\lambda=0,1$

                  \item \begin{enumerate}
                            \item 必要性: 设 $\mathbf{P}$ 是投影矩阵.
                                  设 $\mathbf{u}\in\mathbb{R}^s$, $\mathbf{v}\in\mathbb{R}^t$, $n=s+t$, $\mathbf{u}$ 和 $\mathbf{v}$ 两者之间任意两维相互独立.

                                  设 $\mathbf{P}\mathbf{x}=\mathbf{v}$, $\mathbf{x}=\mathbf{u}+\mathbf{v}$.\

                                  $$\mathbf{P}^2\mathbf{x}=\mathbf{P}\mathbf{v}=\mathbf{0}+\mathbf{v}$$
                                  所以 $\mathbf{P}^2=\mathbf{P}$
                            \item 充分性: $\mathbf{P}^2=\mathbf{P}$, $\mathbf{P}$ 是对称矩阵.
                            \item $$\mathbf{x}=(\mathbf{x}-\mathbf{P}\mathbf{x})+\mathbf{P}\mathbf{x},\forall \mathbf{x}\in\mathbb{R}^n$$
                                  显然 $\{\mathbf{P}\mathbf{x}\}=\mathcal{C}(\mathbf{P})$

                                  与此同时, 若 $\mathbf{y}\in\{\mathbf{x}-\mathbf{P}\mathbf{x}\}$, 则 $\mathbf{P}\mathbf{y}=\mathbf{0}$, 所以 $\{\mathbf{x}-\mathbf{P}\mathbf{x}\}\subseteq\mathcal{N}(\mathbf{P})$

                                  另一方面, 若 $\mathbf{x}\in\mathcal{N}(\mathbf{P})$, 则 $\mathbf{P}(\mathbf{x}-\mathbf{P}\mathbf{x})=\mathbf{0}$, 所以 $\mathcal{N}(\mathbf{P})\subseteq\{\mathbf{x}-\mathbf{P}\mathbf{x}\}$

                                  所以 $\mathcal{N}(\mathbf{P})=\{\mathbf{x}-\mathbf{P}\mathbf{x}\}$

                                  所以 $\mathcal{N}(\mathbf{P})\bigoplus\mathcal{C}(\mathbf{P})=\mathbb{R}^n$,

                                  由定义可知 $\mathbf{P}$ 为投影矩阵.
                        \end{enumerate}

              \end{enumerate}
    \end{enumerate}
\end{solution}
\newpage




% done
\begin{exercise}[\textnormal{5pts}]
    Given $\mathbf{X}=(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_d)\in \mathbb{R}^{n\times d}$, please show that:
    \begin{enumerate}
        \item $\mathbf{X}^{\top}\mathbf{X}$ is always positive semi-definite. Moreover, $\mathbf{X}^{\top}\mathbf{X}$ is positive definite if and only if $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_d$ are linearly independent.
        \item $\mathbf{X}^{\top}\mathbf{X} + \lambda \mathbf{I}$ is always invertible, where $\lambda>0$ and $\mathbf{I}\in \mathbb{R}^{d\times d}$ is an identity matrix.
    \end{enumerate}


\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item $\forall \mathbf{y}\in\mathbb{R}^{d\times 1}$
              $$\mathbf{y}^\top \left( \mathbf{X}^\top \mathbf{X} \right) \mathbf{y}=\left(\mathbf{X}\mathbf{y}\right)^\top \left(\mathbf{X}\mathbf{y}\right)\geq 0$$
              所以 $\mathbf{X}^\top\mathbf{X}$ 半正定.
        \item 因为 $\mathbf{X}^\top\mathbf{X}$ 半正定, 所以 $\mathbf{X}^\top\mathbf{X}$ 的特征值 $\mu_i \ge 0, i=1,2,\cdots,d$, 对应的特征向量为 $\mathbf{v}_1,\mathbf{v}_2,\cdots,\mathbf{v}_d$.

              所以
              $$\left( \mathbf{X}^\top\mathbf{X}+\lambda \mathbf{I} \right)\mathbf{v}_i=(\mu_i+\lambda_i)\mathbf{v}_i$$

              所以 $\mu_i+\lambda_i> 0$ 是 $\mathbf{X}^\top\mathbf{X}+\lambda \mathbf{I}$ 的特征值.

              所以 $\mathbf{X}^\top\mathbf{X}+\lambda \mathbf{I}$ 是可逆的.
    \end{enumerate}
\end{solution}


\newpage


% done
\begin{exercise}[Linear Regression by Maximum Likelihood  \textnormal{10pts}]\label{Exe5}
    Suppose that the samples $\{(\mathbf{x}_i,y_i)\}^n_{i=1}$ are i.i.d., where $\mathbf{x}_i =(x_{i,1}, \dots, x_{i,d})^{\top} \in \mathbb{R}^d$  and $y_i \in \mathbb{R}$. For any $i\in \{1,\dots, n\}$, we assume that
    $$y_i =  w_0 + w_1 x_{i,1} +\dots + w_d x_{i,d} + \epsilon_i,$$
    where $\mathbf{w} = (w_0,w_1,\dots,w_d)^{\top}\in \mathbb{R}^{d+1}$ and $\epsilon_i\sim \mathcal{N}(0,\sigma^2)$. For simplicity, we define $\bar{\mathbf{x}}_i = (1, x_{i,1}, \dots, x_{i,d})^\top$, $ \mathbf{X}=(\bar{\mathbf{x}}_1,\dots,\bar{\mathbf{x}}_n)^\top$, and $\mathbf{y}=(y_1,\dots,y_n)^\top$, where $\mathbf{X}$ has full rank.
    \begin{enumerate}
        \item Please find the maximum likelihood estimation (MLE) $\hat{\mathbf{w}}$ of the weights $\mathbf{w}$. Specifically, please give the expression of $w_0$.
        \item Please find the MLE of $\sigma$.
    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item $y_i$ 的概率密度函数为
              $$f(y_i) =\cfrac{1}{\sqrt{2\pi\sigma^2}}\exp \left( -\cfrac{\left( y_i - \mathbf{w}^\top \overline{\mathbf{x}}_i \right)^2}{2\sigma^2} \right)$$
              $$\begin{aligned}
                      L & =\sum_{i=1}^n \ln f(y_i)                                                                                                        \\
                        & =-\cfrac{n}{2}\ln 2\pi -n\sigma-\cfrac{1}{2\sigma^2}\sum_{i=1}^n \left( y_i - \mathbf{w}^\top \overline{\mathbf{x}}_i \right)^2 \\
                        & =-\cfrac{n}{2}\ln 2\pi -n\sigma-\cfrac{1}{2\sigma^2} \|\mathbf{y}-\overline{\mathbf{X}}\mathbf{w}\|_2^2
                  \end{aligned}$$
              目标为
              $$\argmax_{\mathbf{w}} L$$
              $$\nabla_{\mathbf{w}} L=\cfrac{1}{\sigma^2}\overline{\mathbf{X}}^\top \left( \mathbf{y}-\overline{\mathbf{X}}\mathbf{w} \right)=\mathbf{0}$$
              所以 $$\hat{\mathbf{w}}=\left(\overline{\mathbf{X}}^\top \overline{\mathbf{X}}\right)^{-1} \overline{\mathbf{X}}^\top \mathbf{y}$$
                $$\hat{w}_0=\cfrac{\sum_{i=1}^{d+1} \left( \sum_{j=1}^n \overline{x}_{ji} \sum_{j=1}^n(\overline{x}_{ji} y_{j}) \right) }{\left| \overline{\mathbf{X}}^\top \overline{\mathbf{X}} \right|}$$
        \item 目标为 $$\argmax_\sigma L$$
              $$\nabla_\sigma L=\cfrac{\|\mathbf{y}-\overline{\mathbf{X}}\mathbf{w}\|_2^2}{\sigma}-n=0$$
              所以 $$\hat{\sigma}=\cfrac{1}{n}\|\mathbf{y}-\overline{\mathbf{X}}\mathbf{w}\|_2^2$$
              用 $\hat{\mathbf{w}}$ 代替 $\mathbf{w}$
              $$\hat{\sigma}=\cfrac{1}{n}\|\mathbf{y}-\overline{\mathbf{X}}\left(\overline{\mathbf{X}}^\top \overline{\mathbf{X}}\right)^{-1} \overline{\mathbf{X}}^\top \mathbf{y}\|_2^2$$
    \end{enumerate}
\end{solution}

\newpage



% done
\begin{exercise}[Multiple outputs linear regression \textnormal{5pts}]
    Suppose that the samples $\{(\mathbf{x}_i,\mathbf{y}_i)\}^n_{i=1}$ are i.i.d., where $\mathbf{x}_i =(x_{i,1}, \dots, x_{i,d})^{\top} \in \mathbb{R}^{d}$  and $\mathbf{y}_i = (y_{i,1}, \dots, y_{i,p})^{\top} \in \mathbb{R}^{p}$. We define $\bar{\mathbf{x}}_i = (1 , x_{i,1} ,\dots , x_{i,d})^{\top} $.  We assume that
    \begin{align*}
        p(\mathbf{y}_i|\mathbf{x}_i, \mathbf{W}, \sigma ) = \mathcal{N} ( \mathbf{W}^{\top} \bar{\mathbf{x}}_i  , \sigma^2 \mathbf{I}),
    \end{align*}
    where $\mathbf{W} \in \mathbb{R}^{(d+1) \times p}$ and $\mathbf{I} \in \mathbb{R}^{p \times p}$ is an identity matrix.
    For simplicity, we assume that $\mathbf{X} = (\bar{\mathbf{x}}_1 , \dots, \bar{\mathbf{x}}_n)^{\top}$ has full rank.

    \begin{enumerate}
        \item Please find the maximum likelihood estimation (MLE) $\hat{\mathbf{W}}$ of the weights $\mathbf{W}$.
        \item Please find the relation between $\hat{\mathbf{W}}$ and $\hat{\mathbf{w}}$ in Exercise \ref{Exe5}.
    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item $\mathbf{y}_i$ 的概率密度函数为
              $$\begin{aligned}
                      f(\mathbf{y}_i)
                       & =\cfrac{1}{\sqrt{(2\pi)^p|\sigma^2\mathbf{I}|}}\exp \left( -\cfrac{1}{2} \left( \mathbf{y}_i - \mathbf{W}^\top \overline{\mathbf{x}}_i \right)^\top \left(\sigma^2 \mathbf{I}\right)^{-1} \left( \mathbf{y}_i - \mathbf{W}^\top \overline{\mathbf{x}}_i \right) \right) \\
                       & =\cfrac{1}{\sqrt{(2\pi\sigma^2)^p}}\exp \left( -\cfrac{1}{2\sigma^2} \| \mathbf{y}_i - \mathbf{W}^\top \overline{\mathbf{x}}_i \|_2^2 \right)
                  \end{aligned}$$
              $$\begin{aligned}
                      L & =\sum_{i=1}^n \log f(\mathbf{y}_i)                                                                                                  \\
                        & =-\cfrac{np}{2}\ln 2\pi-np\sigma -\cfrac{1}{2\sigma^2}\sum_{i=1}^n \| \mathbf{y}_i - \mathbf{W}^\top \overline{\mathbf{x}}_i \|_2^2 \\
                        & =-\cfrac{np}{2}\ln 2\pi-np\sigma -\cfrac{1}{2\sigma^2}\|\mathbf{Y}-\overline{\mathbf{X}}\mathbf{W}\|_2^2
                  \end{aligned}$$
              其中 $\mathbf{Y}=(\mathbf{y}_1,\mathbf{y}_2,\cdots,\mathbf{y}_n)^\top$, $\overline{\mathbf{X}}=(\overline{\mathbf{x}}_1,\overline{\mathbf{x}}_2,\cdots,\overline{\mathbf{x}}_n)^\top$.

              目标为
              $$\argmax_{\mathbf{W}} L$$
              $$\nabla_{\mathbf{W}} L=\cfrac{1}{\sigma^2}\overline{\mathbf{X}}^\top\left( \mathbf{Y}-\overline{\mathbf{X}}\mathbf{W} \right)=\mathbf{0}$$
              $$\mathbf{W}=\left(\overline{\mathbf{X}}^\top \overline{\mathbf{X}}\right)^{-1}\overline{\mathbf{X}}^\top\mathbf{Y}$$
        \item $p=1$ 时, $\hat{\mathbf{W}}=\hat{\mathbf{w}}$.
    \end{enumerate}
\end{solution}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
