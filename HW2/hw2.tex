%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages} 
\usepackage{enumitem}
\usepackage{ctex}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\pagestyle{fancy}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Oct. 05, 2019}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Oct. 12, 2019}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{2}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Jiahuan Yu}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB17121687}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\dom}{ \textbf{dom}  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
    {\bf\large Introduction to Machine Learning}\\
    {Fall 2019}\\
    University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name
\hfill
ID: \id
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please present your solutions step by step.

\begin{exercise}[Lipschitz Continuity \textnormal{10pts}]
    Suppose that $f:\mathbb{R}^n\rightarrow\mathbb{R}$ is twice continuously differentiable, and the gradient of $f$ is Lipschitz continuous, i.e.,
    \begin{align*}
        \|\nabla f(x)-\nabla f(y)\|_2\le L\|x-y\|_2, \forall\,x,y\in\mathbb{R}^n,
    \end{align*}
    where $L>0$ is the Lipschitz constant. Please find the relation between $L$ and the largest eigenvalue of $\nabla^2 f(x)$.
\end{exercise}
\begin{solution}
    设
    $$g(t)=\nabla f(x+tcy)$$
    所以
    $$\nabla g(t)= \left[ \nabla^2 f(x+cty) \right] cy$$
    所以
    $$\begin{aligned}
            \nabla f(x+cy)-\nabla f(x)
             & = g(1)-g(0)                              \\
             & = \nabla g(\xi) (1-0)                    \\
             & = \left[ \nabla^2 f(x+c\xi y) \right] cy \\
        \end{aligned}$$
    因为
    $$\|\nabla f(x+cy)-\nabla f(x)\|_2 \le L\|cy\|_2 = Lc \|y\|_2$$
    所以
    $$\|\left[ \nabla^2 f(x+c\xi y) \right] y\|_2 \le L\|y\|_2$$
    令 $c=0$, 所以
    $$\|\left[ \nabla^2 f(x) \right] y\|_2 \le L\|y\|_2$$
    若 $y$ 是特征向量，$\lambda$ 是特征值，则
    $$\|\left[ \nabla^2 f(x) \right] y\|_2 = \lambda \|y\|_2$$
    所以
    $$\lambda \le L$$
\end{solution}
\newpage



\begin{exercise}[Gradient Descent for Convex Optimization Problems \textnormal{20pts}]
    Consider the following problem
    \begin{align}\label{prob:c_min}
        \min_{x}f(x),
    \end{align}
    where $f$ is convex and its gradient is Lipschitz continuous with constant $L>0$. Assume that $f$ can attain its minimum.
    \begin{enumerate}
        \item Show that the optimal set $\mathcal{C}=\{y:f(y)=\min_{x}f(x)\}$ is convex.
        \item Suppose that $d(x,\mathcal{C})=\inf_{z\in\mathcal{C}}\|x-z\|_2$. Consider the problem (\ref{prob:c_min}) and the sequence generated by the gradient descent algorithm. Show that $d(x_k,\mathcal{C})\rightarrow 0$ as $k\rightarrow\infty$.
    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item 设 $y_1,y_2\in \mathcal{C}$, 有 $f(x)=f(y)$.

              因为 $f$ 是凸函数，所以 $\forall\theta\in [0,1]$
              $$f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)=f(x)=f(y)$$
              所以
              $$\theta x+(1-\theta)y \in \mathcal{C}$$
              所以 $\mathcal{C}$ 是凸集。
        \item 因为 $f$ 是凸函数，所以
              $$\mathcal{C}=\{ y:\|\nabla f(y)\|_2=0 \}$$
              $$\begin{aligned}
                      f(y)-f(x)
                       & = \int_x^y \nabla f(x){\rm d}x                                        \\
                       & = \int_0^1 \langle \nabla f(x+t(y-x)),y-x \rangle {\rm d}t            \\
                       & = \langle\nabla f(x),y-x \rangle
                      + \int_0^1 \langle \nabla f(x+t(y-x))-\nabla f(x),y-x \rangle {\rm d}t   \\
                       & \le \langle\nabla f(x),y-x \rangle
                      + \int_0^1 \| \nabla f(x+t(y-x))-\nabla f(x) \|_2^2 \|y-x\|_2^2 {\rm d}t \\
                       & \le \langle\nabla f(x),y-x \rangle +L\|y-x\|_2^2 \int_0^1 {t{\rm d}t} \\
                       & = \langle \nabla f(x),y-x \rangle+\cfrac{L}{2}\|y-x\|_2^2
                  \end{aligned}$$
              设 $y=x_{k+1}$, $x=x_k$, 则
              $$\begin{aligned}
                      f(x_{k+1})-f(x_k)
                       & \le \langle \nabla f(x_k),x_{k+1}-x_k \rangle+\cfrac{L}{2}\|x_{k+1}-x_k\|_2^2                 \\
                       & = \langle \nabla f(x_k),-\alpha\nabla f(x_k) \rangle+\cfrac{L}{2}\|-\alpha\nabla f(x_k)\|_2^2 \\
                       & =-\alpha (1-\cfrac{L\alpha}{2}) \|\nabla f(x_k)\|_2^2
                  \end{aligned}$$
              所以
              $$\|\nabla f(x_k)\|_2^2\le \cfrac{f(x_k)-f(x_{k+1})}{\alpha(1-\frac{L\alpha}{2})}$$
              所以
              $$\sum_{k=1}^{\infty}{\|\nabla f(x_k)\|_2^2} \le \cfrac{f(x_0)-f(x^\star)}{\alpha(1-\frac{L\alpha}{2})}$$
              右边的式子是有限的，所以
              $$\lim_{k\to\infty} \|\nabla f(x_k)\|_2=0$$
              所以
              $$\lim_{k\to\infty}{d(x_k,\mathcal{C})}=0$$
    \end{enumerate}
\end{solution}
\newpage



\begin{exercise}[Gradient Descent for Strongly Convex Optimization Problems \textnormal{50pts}]
    A function $f$ is strongly convex with parameter $\mu$ if $f(x)-\cfrac{\mu}{2}\|x\|_2^2$ is convex.
    \begin{enumerate}
        \item Show that a continuously differentiable function $f$ is strongly convex if and only if
              \begin{align*}
                  f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle+\cfrac{\mu}{2}\|y-x\|_2^2, \forall\, x,y\in\mathbb{R}^n
              \end{align*}
        \item Suppose that $f$ is twice differentiable. Please find the relation between $\mu$ and the smallest eigenvalue of $\nabla^2f(x)$.
    \end{enumerate}

    \noindent Consider the following problem
    \begin{align}\label{prob:sc_min}
        \min_{x}f(x),
    \end{align}
    where $f$ is strongly convex with convexity parameter $\mu>0$ and its gradient is Lipschitz continuous with constant $L>0$.
    \begin{enumerate}[resume]
        \item Show that the problem (\ref{prob:sc_min}) admits a unique solution.
        \item Show that
              \begin{align*}
                  f(y)\ge f(x)-\cfrac{1}{2\mu}\|\nabla f(x)\|_2^2, \forall\, x,y.
              \end{align*}
        \item Consider the problem (\ref{prob:sc_min}) and the sequence generated by the gradient descent algorithm. Suppose that $x^*$ is the solution to the problem \ref{prob:sc_min}. Show that
              \begin{align*}
                  f(x_k)-f(x^*)\le (1-\mu\alpha(2-L\alpha))^k(f(x_0)-f(x^*)).
              \end{align*}
              Find the range of $\alpha$ such that the function values $f(x_k)$ converge linearly to  $f(x^*)$.
    \end{enumerate}
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item \begin{enumerate}
                  令 $g(x)=f(x)-\cfrac{\mu}{2}\|x\|_2^2$
                  \item 充分性：设 $f$ 是强凸函数, 则 $g(x)$ 为凸函数，所以
                        $$g(y)\ge g(x)+\langle \nabla g(x),y-x \rangle$$
                        即
                        $$\begin{aligned}
                                f(y)-\cfrac{\mu}{2}\|y\|_2^2
                                 & \ge f(x)-\cfrac{\mu}{2}\|x\|_2^2
                                + \langle \nabla f(x)-\cfrac{\mu}{2}\nabla \|x\|_2^2,y-x \rangle \\
                                 & = f(x)-\cfrac{\mu}{2}\|x\|_2^2
                                + \langle \nabla f(x)-\mu x,y-x \rangle                          \\
                                 & = f(x)-\cfrac{\mu}{2}\|x\|_2^2
                                + \langle \nabla f(x),y-x \rangle -\mu\langle x,y-x \rangle      \\
                                 & = f(x)+\langle \nabla f(x),y-x \rangle
                                +\cfrac{\mu}{2}\|x\|_2^2 - \mu \langle x,y \rangle
                            \end{aligned}$$
                        所以
                        $$\begin{aligned}
                                f(y)
                                 & \ge f(x)+\langle \nabla f(x),y,x \rangle
                                +\cfrac{\mu}{2}\left( \|x\|_2^2 + \|y\|_2^2 -2\langle x,y \rangle \right) \\
                                 & = f(x)+\langle \nabla f(x),y,x \rangle
                                + \cfrac{\mu}{2} \|y-x\|_2^2
                            \end{aligned}$$
                        得证。

                  \item 必要性：设 $ f(y)\ge f(x)+\langle\nabla f(x),y-x\rangle+\cfrac{\mu}{2}\|y-x\|_2^2$.

                        仿照充分性的证明，逆向推导，可以得到
                        $$g(y)\ge g(x)+\langle \nabla g(x),y-x \rangle$$
                        所以 $g(x)$ 是凸函数，$f(x)$ 是强凸函数。

              \end{enumerate}
        \item 由上题可知
              $$f(y)-f(x) \ge \langle\nabla f(x),y-x\rangle+\cfrac{\mu}{2}\|y-x\|_2^2$$
              另一方面
              $$\begin{aligned}
                      f(y)-f(x)
                       & = \int_x^y{\nabla f(x){\rm d}x}                                    \\
                       & = \langle\nabla f(x),y-x \rangle
                      +\int_0^1{\langle \nabla f(x+t(y-x))-\nabla f(x),y-x \rangle}{\rm d}t \\
                       & \le \langle\nabla f(x),y-x \rangle
                      +\int_0^1 \| \nabla f(x+t(y-x))-\nabla f(x) \|_2 \|y-x\|_2 {\rm d}t   \\
                       & = \langle\nabla f(x),y-x \rangle
                      +\int_0^1 \| \nabla^2 f(\xi)\cdot(y-x) \|_2 \|y-x\|_2 t{\rm d}t       \\
                  \end{aligned}$$
              其中 $\xi$ 处于 $x$ 和 $x+t(y-x)$ 之间。

              比较上述两个式子，得
              $$\int_0^1 \| \nabla^2 f(\xi)\cdot(y-x) \|_2 \|y-x\|_2 t{\rm d}t
                  \ge \cfrac{\mu}{2}\|y-x\|_2^2$$
              积分化简得
              $$\|\nabla^2 f(\xi)\cdot(y-x)\|_2^2 \ge \mu \|y-x\|_2^2$$
              即
              $$\|\nabla^2 f(\xi)\cdot x\|_2^2 \ge \mu \|x\|_2^2, \forall x, \xi$$
              令 $x$ 为 $\nabla^2 f(\xi)$ 的特征向量，$\lambda$ 为特征值，有
              $$\nabla^2 f(\xi)\cdot x=\lambda x$$
              所以
              $$\mu \le \lambda$$

              即 $\mu$ 不大于 $\nabla^2 f(x)$ 的最小特征值

        \item 假设问题(\ref{prob:sc_min})的解不唯一，设解集为 $\mathcal{C}$.

              由 Exercise 2.1 可知，$\mathcal{C}$ 是凸集。

              设 $x\in\mathcal{C}$ 时
              $$f(x)=g(x)+\cfrac{\mu}{2}\|x\|_2^2=M$$
              其中 $M$ 是常数。

              所以 $x\in\mathcal{C}$ 时
              $$g(x)=M-\cfrac{\mu}{2}\|x\|_2^2$$
              与 $g(x)$ 是凸函数矛盾。

              所以解唯一。
        \item 因为
              $$\langle\nabla f(x),y-x \rangle+\cfrac{\mu}{2}\|y-x\|_2^2+\cfrac{1}{2\mu}\|x\|_2^2
                  = \|\sqrt{\cfrac{1}{2\mu}}\nabla f(x)+\sqrt{\cfrac{\mu}{2}}(y-x)\|_2^2
                  \ge 0$$
              所以
              $$\langle\nabla f(x),y-x \rangle+\cfrac{\mu}{2}\|y-x\|_2^2
                  \ge -\cfrac{1}{2\mu}\|x\|_2^2$$
              又因为 Exercise 3.1, 有
              $$f(y)-f(x)\ge\langle\nabla f(x),y-x\rangle+\cfrac{\mu}{2}\|y-x\|_2^2$$
              所以
              $$ f(y)\ge f(x)-\cfrac{1}{2\mu}\|\nabla f(x)\|_2^2$$
        \item 由 Exercise 2.2 和 Exercise 3.4
              $$\begin{aligned}
                      f(x_{k+1})-f(x_k)
                       & =-\alpha (1-\cfrac{L\alpha}{2}) \|\nabla f(x_k)\|_2^2 \\
                       & \le \mu\alpha(2-L\alpha) (f(x^\star)-f(x_k))
                  \end{aligned}$$
              所以
              $$\begin{aligned}
                      f(x_k)-f(x^\star)
                       & \le f(x_{k-1})-f(x^\star) + \mu\alpha(2-L\alpha) (f(x^\star)-f(x_{k-1})) \\
                       & = (1-\mu\alpha(2-L\alpha)) (f(x_{k-1})-f(x^\star))
                  \end{aligned}$$
              所以
              $$ f(x_k)-f(x^\star) \le (1-\mu\alpha(2-L\alpha))^k (f(x_0)-f(x^\star)) $$
    \end{enumerate}
\end{solution}
\newpage



\begin{exercise}[Programming Exercise \textnormal{20pts}]
    We provide you with a data set, where the number of samples $n$ is $16087$ and the number of features $d$ is $10013$. Suppose that $\textbf{X}\in\mathbb{R}^{n\times d}$ is the input feature matrix and $\textbf{y}\in\mathbb{R}^n$ is the corresponding response vector. We use the linear model to fit the data, and thus we can formulate the optimization problem as
    \begin{align}\label{prob:lsm}
        \arg\min_{\textbf{w}} \frac{1}{n}\|\textbf{y}-\bar{\textbf{X}}\textbf{w}\|_2^2,
    \end{align}
    where $\bar{\textbf{X}}=(\textbf{1},\textbf{X})\in\mathbb{R}^{n\times(d+1)}$ and $\textbf{w}=(w_0,w_1,\dots,w_n)^\top\in\mathbb{R}^{d+1}$.
    Finish the following exercises by programming. You can use your favorite programming language.
    \begin{enumerate}
        \item Normalize the columns $\textbf{x}_i$ of $\bar{\textbf{X}}$ ($2\le i\le d+1$) as follows:
              \begin{align*}
                  \textbf{x}_{ij}\leftarrow\frac{\textbf{x}_{ij}-\min (\textbf{x}_i)}{\max (\textbf{x}_i)-\min (\textbf{x}_i)},
              \end{align*}
              where $\textbf{x}_{ij}$ denote thes $j$th entry of $\textbf{x}_i$. Use the normalized $\bar{\textbf{X}}$ in the following exercises.
        \item Use the closed form solution to solve the problem (\ref{prob:lsm}), and get the solution $\textbf{w}_0^*$.
        \item Use the gradient descent algorithm to solve the problem (\ref{prob:lsm}). Stop the iteration until $\frac{1}{n}|f(\textbf{w}_k)-f(\textbf{w}_0^*)|<0.1$, where $f(\textbf{w}) = \frac{1}{n}\|\textbf{y}-\bar{\textbf{X}}\textbf{w} \|_2^2$. Plot $f(\textbf{w}_k)$ versus the iteration step $k$.
    \end{enumerate}
    Compare the time cost of the two approaches in 2 and 3.

\end{exercise}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
