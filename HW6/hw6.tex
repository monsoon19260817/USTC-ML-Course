%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Template for homework of Introduction to Machine Learning.
%
%  Fill in your name, lecture number, lecture date and body
%  of homework as indicated below.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[11pt,letter,notitlepage]{article}
%Mise en page
\usepackage[left=2cm, right=2cm, lines=45, top=0.8in, bottom=0.7in]{geometry}
\usepackage{ctex}
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{pdfpages} 
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\headrulewidth}{1.5pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand\Loadedframemethod{TikZ}
\usepackage[framemethod=\Loadedframemethod]{mdframed}

\usepackage{amssymb,amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\newtheorem{lemma}{Lemma}

\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}

\usepackage{graphicx} % more modern
\usepackage{subfigure}
\usepackage{threeparttable}

%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Define math operator %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{\bf argmin}
\DeclareMathOperator*{\argmax}{\bf argmax}
\DeclareMathOperator*{\relint}{\bf relint\,}
\DeclareMathOperator*{\dom}{\bf dom\,}
\DeclareMathOperator*{\intp}{\bf int\,}
\DeclareMathOperator*{\tr}{\bf tr\,}
%%%%%%%%%%%%%%%%%%%%%%%


\setlength{\topmargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\headheight}{0pt}

\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\pagestyle{fancy}
%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{exercise}{\textbf{Exercise}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Problem environment %%
%%%%%%%%%%%%%%%%%%%%%%%%
\mdtheorem[
topline=false,
rightline=false,
leftline=false,
bottomline=false,
leftmargin=-10,
rightmargin=-10
]{problem}{\textbf{Problem}}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Exercise environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
%% Define the Solution Environment %%
%%%%%%%%%%%%%%%%%%%%%%%
\declaretheoremstyle
[
spaceabove=0pt, 
spacebelow=0pt, 
headfont=\normalfont\bfseries,
notefont=\mdseries, 
notebraces={(}{)}, 
headpunct={:\quad}, 
headindent={},
postheadspace={ }, 
postheadspace=4pt, 
bodyfont=\normalfont, 
qed=$\blacksquare$,
preheadhook={\begin{mdframed}[style=myframedstyle]},
	postfoothook=\end{mdframed},
]{mystyle}

\declaretheorem[style=mystyle,title=Solution,numbered=no]{solution}
\mdfdefinestyle{myframedstyle}{%
	topline=false,
	rightline=false,
	leftline=false,
	bottomline=false,
	skipabove=-6ex,
	leftmargin=-10,
	rightmargin=-10}
%%%%%%%%%%%%%%%%%%%%%%%
%% End of the Solution environment %%
%%%%%%%%%%%%%%%%%%%%%%%

%% Homework info.
\newcommand{\posted}{\text{Nov. 27, 2019}}       			%%% FILL IN POST DATE HERE
\newcommand{\due}{\text{Dec. 11, 2019}} 			%%% FILL IN Due DATE HERE
\newcommand{\hwno}{\text{6}} 		           			%%% FILL IN LECTURE NUMBER HERE


%%%%%%%%%%%%%%%%%%%%
%% Put your information here %%
%%%%%%%%%%%%%%%%%%%
\newcommand{\name}{\text{Jiahuan Yu}}  	          			%%% FILL IN YOUR NAME HERE
\newcommand{\id}{\text{PB17121687}}		       			%%% FILL IN YOUR ID HERE
%%%%%%%%%%%%%%%%%%%%
%% End of the student's info %%
%%%%%%%%%%%%%%%%%%%


\newcommand{\proj}[2]{\textbf{P}_{#2} (#1)}
\newcommand{\lspan}[1]{\textbf{span}  (#1)  }
\newcommand{\rank}[1]{ \textbf{rank}  (#1)  }
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\lhead{
	\textbf{\name}
}
\rhead{
	\textbf{\id}
}
\chead{\textbf{
		Homework \hwno
}}


\begin{document}
\vspace*{-4\baselineskip}
\thispagestyle{empty}


\begin{center}
    {\bf\large Introduction to Machine Learning}\\
    {Fall 2019}\\
    University of Science and Technology of China
\end{center}

\noindent
Lecturer: Jie Wang  			 %%% FILL IN LECTURER HERE
\hfill
Homework \hwno
\\
Posted: \posted
\hfill
Due: \due
\\
Name: \name
\hfill
ID: \id
\hfill

\noindent
\rule{\textwidth}{2pt}

\medskip





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BODY OF HOMEWORK GOES HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Notice, }to get the full credits, please show your solutions step by step.

\begin{exercise}[\textnormal{10pts}]
    Show that $(1-\epsilon)^m\leq e^{-m\epsilon}$, where $m\in \mathbb{N}^+$ and $0\le \epsilon<1$.
\end{exercise}

\begin{solution}
    只要证明
    $$m\ln(1-\epsilon)\leq -m\epsilon$$
    即
    $$\ln(1-\epsilon)\leq -\epsilon$$
    设
    $$f(\epsilon)=\ln(1-\epsilon)+\epsilon$$
    $$f'(\epsilon)=1-\cfrac{1}{1-\epsilon}=-\cfrac{\epsilon}{1-\epsilon}$$
    因为 $0\leq\epsilon <1$
    所以 $$f'(\epsilon)\leq0$$
    所以 $$f(\epsilon)\leq f(0)=0$$
    所以 $$\ln(1-\epsilon) \leq -\epsilon$$
    得证。
\end{solution}

\newpage


\begin{exercise}[Markov inequality \textnormal{10pts}]
    Let $X$ be a nonnegative random variable on $\mathbb{R}$. Then, for all $t>0$, show that
    $$\mathbf{P}(X\geq t)\leq \frac{\mathbf{E}[X]}{t}.$$
    You can assume that $X$ is a continuous random variable.
\end{exercise}

\begin{solution}
    设 $X$ 的概率密度函数为 $f(x)$, 有 $f(x)\geq 0$.
    $$\begin{aligned}
            \mathbf{E}[X]
             & = \int_{-\infty}^{\infty}{x f(x) \text{d}x}                                 \\
             & = \int_{-\infty}^{0}{x f(x) \text{d}x} + \int_{0}^{\infty}{xf(x) \text{d}x} \\
             & \geq \int_{0}^{\infty}{x f(x) \text{d}x}                                    \\
             & \geq \int_{t}^{\infty}{x f(x) \text{d}x}                                    \\
             & \geq \int_{t}^{\infty}{t f(x) \text{d}x}                                    \\
             & = t \int_{t}^{\infty}{ f(x) \text{d}x}                                      \\
             & = t \mathbf{P}(X\geq t)
        \end{aligned}$$
    所以有
    $$\mathbf{P}(X \geq t)\leq\cfrac{\mathbf{E}[X]}{t}$$
\end{solution}

\newpage

\begin{exercise}[VC-dimension \textnormal{10pts}]
    Assume that the instance space $X=\mathbb{R}^2$ and the hypothesis space $H$ be the set of all linear threshold functions defined on $\mathbb{R}^2$. Find $VC(H)$ and prove it.
\end{exercise}

\begin{solution}
    $VC(H)=3$, 证明如下：

\end{solution}

\newpage

\begin{exercise}[Learning intervals \textnormal{20pts}]
    Let the target concept class be $C=\{[a,b]:a<b, a,b\in\mathbb{R}\}$ and the hypotheses class $H=C$, and the version space be $VS_{H,D}$. Each $c\in C$ labels the points inside the interval positive and the others negative. A consistent learner will pick a consistent hypothesis---if any---$h\in H$ according to a set of i.i.d. samples $\{(x_1,c(x_1)),(x_2,c(x_2)),\ldots,(x_m,c(x_m))\}$ that obey an unknown absolute continuous distribution $\mathcal{D}$. $\mathcal{D}$'s p.d.f. is $p(x)$. Please find
    $$\mathbf{P}[\exists\, h\in VS_{H,D} \mbox{ and } error_{\mathcal{D}}(h)>\epsilon],$$
    and the corresponding sample complexity.
\end{exercise}


\begin{solution}

\end{solution}

\newpage

\begin{exercise}[Basic Matrix Manipulations  \textnormal{20pts}]
    For an arbitrary matrix $M$, we denote its $i^{th}$ row, $j^{th}$ column, and $(i,j)^{th}$ entry by $\mathbf{m}_{i,*}$, $\mathbf{m}_{*,j}$, and $m_{i,j}$, respectively.
    \begin{enumerate}
        \item Suppose that $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{m\times d}$, $C\in\mathbb{R}^{d\times n}$, and $A=BC$. Show that
              \begin{align*}
                  A=\sum_{\ell=1}^d\mathbf{b}_{*,\ell}\mathbf{c}_{\ell,*}.
              \end{align*}

        \item Suppose that $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{m\times p}$, $C\in\mathbb{R}^{p\times q}$, $D\in\mathbb{R}^{q\times n}$, and $A=BCD$. Show that
              \begin{align*}
                  A=\sum_{i=1}^p\sum_{j=1}^qc_{i,j}\mathbf{b}_{*,i}\mathbf{d}_{j,*}.
              \end{align*}
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item $$A_{i,j}=\sum_{\ell=1}^{d}{b_{i,\ell}c_{\ell,j}}$$
              而 $$\left( \mathbf{b}_{*,\ell}\mathbf{c}_{\ell,*} \right)_{i,j}
                  =b_{i,\ell}c_{\ell,j}$$
              所以有 $$A=\sum_{\ell=1}^d\mathbf{b}_{*,\ell}\mathbf{c}_{\ell,*}$$
        \item $$A= BCD  = \left( \sum_{i=1}^p\mathbf{b}_{*,i}\mathbf{c}_{i,*} \right) D$$
              而 $$\left( \sum_{i=1}^p\mathbf{b}_{*,i}\mathbf{c}_{i,*} \right)_{*,j}
                  =\sum_{i=1}^p \left( \mathbf{b}_{*,i}\mathbf{c}_{i,*}\right)_{*,j}
                  =\sum_{i=1}^p \mathbf{b}_{*,i}\mathbf{c}_{i,j}$$
              所以 $$A
                  =\left( \sum_{i=1}^p\mathbf{b}_{*,i}\mathbf{c}_{i,*} \right) D
                  =\sum_{j=1}^q \left( \left( \sum_{i=1}^p\mathbf{b}_{*,i}\mathbf{c}_{i,*} \right)_{*,j} \mathbf{d}_{j,*} \right)
                  =\sum_{i=1}^p\sum_{j=1}^qc_{i,j}\mathbf{b}_{*,i}\mathbf{d}_{j,*}$$
    \end{enumerate}
\end{solution}


\newpage

\begin{exercise}[Subspace  \textnormal{90pts}]
    The column space of a matrix $A\in\mathbb{R}^{m\times n}$ is the set
    \begin{align}\label{def:column-space}
        \mathcal{C}(A)=\{\mathbf{y}\in\mathbb{R}^m: \mathbf{y}=A\mathbf{x}, \mathbf{x}\in\mathbb{R}^n\}.
    \end{align}

    \begin{enumerate}
        \item Let $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{n\times p}$, and $C=AB$.
              \begin{enumerate}
                  \item Show that $\mathcal{C}(C)\subseteq\mathcal{C}(A)$.

                  \item Suppose that $B$ is nonsingular, that is, $B$ is invertible. Show that $\mathcal{C}(C)=\mathcal{C}(A)$.
              \end{enumerate}

        \item\label{exercise:subspace-2} Suppose that $A\in\mathbb{R}^{m\times n}$ has full column rank, that is, the column vectors in $A$ are linearly independent. Let $\mathbf{x}\in\mathbb{R}^m$ and
              \begin{align}\label{prob:proj-subspace}
                  P_{\mathcal{C}(A)}(\mathbf{x}):=\argmin_{\mathbf{z}\in\mathbb{R}^m}\,\{\|\mathbf{x}-\mathbf{z}\|_2: \mathbf{z}\in\mathcal{C}(A)\}.
              \end{align}

              \begin{enumerate}
                  % \item Please find $P_{\mathcal{C}(A)}$, which is indeed the projection of $\mathbf{x}$ into the subspace $\mathcal{C}(A)$. 

                  \item Is $P_{\mathcal{C}(A)}$ unique? If so, please justify your answer and find $P_{\mathcal{C}(A)}$; otherwise, please find all the projections.

                  \item What are the coordinates of $P_{\mathcal{C}(A)}$ with respect to the column vectors in $A$? Are the coordinates unique?
              \end{enumerate}


        \item Suppose that the column vectors in $A\in\mathbb{R}^{m\times n}$ are orthonormal.
              \begin{enumerate}
                  \item Please answer the questions in \ref{exercise:subspace-2}.

                  \item Suppose that the column vectors in $\widetilde{A}\in\mathbb{R}^{m\times n}$ are also orthonormal, and $\mathcal{C}(A)=\mathcal{C}(\widetilde{A})$. Show that $P_{\mathcal{C}(A)}(\mathbf{x})=P_{\mathcal{C}(\widetilde{A})}(\mathbf{x})$ for any $\mathbf{x}\in\mathbb{R}^m$.
              \end{enumerate}

        \item Suppose that the column vectors in $A\in\mathbb{R}^{m\times n}$ are linearly dependent.
              \begin{enumerate}
                  \item Is $P_{\mathcal{C}(A)}$ unique? If so, please justify your answer; otherwise, please find all the projections.

                  \item Are the coordinates of $P_{\mathcal{C}(A)}$ with respect to the column vectors in $A$ unique? If so, please justify your answer; otherwise, please find all the possible coordinates.
              \end{enumerate}
              Hint: you may assume that the first $r$ column vectors with $r<n$ are a basis of $\mathcal{C}(A)$.
    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item \begin{enumerate}
                  \item 设 $\mathbf{y}\in\mathcal{C}(C), \mathbf{y} \in\mathbb{R}^m$, 所以有
                        $$\mathbf{y}=AB\mathbf{x}, \mathbf{x} \in\mathbb{R}^p$$
                        所以 $$\mathbf{y}=A(B\mathbf{x}), B\mathbf{x} \in \mathbb{R}^n$$
                        即 $\mathbf{y} \in \mathcal{C}(A)$，所以
                        $$\mathcal{C}(C)\subseteq\mathcal{C}(A)$$
                  \item
                        只需要再证明 $\mathcal{C}(A)\subseteq\mathcal{C}(C)$.
                        同样，设 $\mathbf{y}\in\mathcal{C}(A), \mathbf{y} \in\mathbb{R}^m$, 所以有
                        $$\mathbf{y}=A\mathbf{x}, \mathbf{x} \in\mathbb{R}^n$$
                        所以  $$\mathbf{y}=AB(B^{-1}\mathbf{x}), B^{-1}\mathbf{x} \in \mathbb{R}^p$$
                        即 $\mathbf{y} \in \mathcal{C}(C)$，所以
                        $$\mathcal{C}(A)\subseteq\mathcal{C}(C)$$
                        所以 $$\mathcal{C}(C)=\mathcal{C}(A)$$
              \end{enumerate}
        \item \begin{enumerate}
                  \item $P_{\mathcal{C}(A)}$ 是唯一的，证明如下： \\
                        假设不唯一，设 $\mathbf{z}_1 \neq \mathbf{z}_2, \mathbf{z}_1,\mathbf{z}_2 \in \mathcal{C}(A)$ 且
                        $$\|\mathbf{x}-\mathbf{z}_1\|_2=\|\mathbf{x}-\mathbf{z}_2\|_2
                            =\min_{\mathbf{z}\in\mathbb{R}^m} \|\mathbf{x}-\mathbf{z}\|_2, \mathbf{z}\in \mathcal{C}(A)$$
                        于是
                        $$\begin{aligned}
                                \|\mathbf{x}-\cfrac{\mathbf{z}_1+\mathbf{z}_2}{2}\|_2^2
                                 & = \| \cfrac{\mathbf{x}-\mathbf{z}_1}{2}+\cfrac{\mathbf{x}-\mathbf{z}_2}{2} \|_2^2                             \\
                                 & \leq 2 \left( \|\cfrac{\mathbf{x}-\mathbf{z}_1}{2}\|_2^2 + \|\cfrac{\mathbf{x}-\mathbf{z}_2}{2}\|_2^2 \right) \\
                                 & = \cfrac{\|\mathbf{x}-\mathbf{z}_1\|_2^2+\|\mathbf{x}-\mathbf{z}_2\|_2^2}{2}                                  \\
                                 & = \min_{\mathbf{z}\in\mathbb{R}^m} \|\mathbf{x}-\mathbf{z}\|_2
                            \end{aligned}$$
                        与假设矛盾。所以 $\|\mathbf{x}-\mathbf{z}\|_2$ 取到最小值时 $\mathbf{z}$ 唯一，因此 $P_{\mathcal{C}(A)}$ 唯一。\\
                        设 $\mathbf{z}=A\mathbf{w}, \mathbf{w}\in\mathbb{R}^n$.
                        $$\|\mathbf{x}-\mathbf{z}\|_2^2
                            =\|\mathbf{x}-A\mathbf{w}\|_2^2$$
                        $$\nabla_\mathbf{w} \|\mathbf{x}-A\mathbf{w}\|_2^2
                            =-2A^\top (\mathbf{x}-A\mathbf{w}) =0$$
                        所以 $$A^\top \mathbf{x}=A^\top A \mathbf{w}$$
                        % todo 证明满秩
                        所以 $$\mathbf{w}=\left( A^\top A \right)^{-1}A^\top \mathbf{x}$$
                        所以 $$P_{\mathcal{C}(A)}(\mathbf{x})=A \left( A^\top A \right)^{-1}A^\top \mathbf{x}$$
                        % 所以 $$P_{\mathcal{C}(A)}=A \left( A^\top A \right)^{-1}A^\top$$
                  \item 由 $$P_{\mathcal{C}(A)}(\mathbf{x})=A \left( A^\top A \right)^{-1}A^\top \mathbf{x}$$
                        以 $A$ 的列向量为基，$P_{\mathcal{C}(A)}(\mathbf{x})$ 的坐标为
                        $$\left( A^\top A \right)^{-1}A^\top \mathbf{x}$$
                        % todo
                        这个坐标是唯一的。
              \end{enumerate}
        \item \begin{enumerate}
                  \item 
                  \item
              \end{enumerate}
        \item \begin{enumerate}
                  \item 
                  \item
              \end{enumerate}
    \end{enumerate}
\end{solution}

\newpage

\begin{exercise}[SVD  \textnormal{80pts}]
    Let $A\in\mathbb{R}^{m\times n}$, $\rank{A}=r$, its SVD be $A=U\Sigma V^{\top}$, where we sort the diagonal entries of $\Sigma$ in the descending order $\sigma_1\geq\sigma_2\geq\ldots\geq\sigma_r>0$, and
    \begin{align*}
         & U_1=(\mathbf{u}_{*,1},\mathbf{u}_{*,2},\ldots,\mathbf{u}_{*,r}), U_2=(\mathbf{u}_{*,r+1},\ldots,\mathbf{u}_{*,m}), \\
         & V_1=(\mathbf{v}_{*,1},\mathbf{v}_{*,2},\ldots,\mathbf{v}_{*,r}), V_2=(\mathbf{v}_{*,r+1},\ldots,\mathbf{u}_{*,n}).
    \end{align*}
    We define the column space of a matrix $A$ in (\ref{def:column-space}).
    The null space of $A$ is the set
    \begin{align}\label{def:column-space}
        \mathcal{N}(A)=\{\mathbf{y}\in\mathbb{R}^n: A\mathbf{y}=0\}.
    \end{align}

    \begin{enumerate}
        \item Show that
              \begin{enumerate}
                  \item $P_{\mathcal{C}(A)}(\mathbf{x})=U_1U_1^{\top}\mathbf{x}$;
                  \item $P_{\mathcal{N}(A)}(\mathbf{x})=V_2V_2^{\top}\mathbf{x}$;
                  \item $P_{\mathcal{C}(A^{\top})}(\mathbf{x})=V_1V_1^{\top}\mathbf{x}$;
                  \item $P_{\mathcal{N}(A^{\top})}(\mathbf{x})=U_2U_2^{\top}\mathbf{x}$.
              \end{enumerate}

        \item The Frobenius norm of $A$ is
              \begin{align*}
                  \|A\|_F=\sqrt{\sum_{i=1}^m\sum_{j=1}^na_{i,j}^2}.
              \end{align*}
              \begin{enumerate}
                  \item Show that $\|A\|_F^2=\tr(A^{\top}A)$.

                  \item Let $B\in\mathbb{R}^{m\times n}$. Suppose that $\mathcal{C}(A)\bot\mathcal{C}(B)$, that is,
                        \begin{align*}
                            \langle\mathbf{a},\mathbf{b}\rangle=0,\,\forall\,\mathbf{a}\in\mathcal{C}(A),\,\mathbf{b}\in\mathcal{C}(B).
                        \end{align*}
                        Show that
                        \begin{align*}
                            \|A+B\|_F^2=\|A\|_F^2+\|B\|_F^2.
                        \end{align*}
              \end{enumerate}

        \item Please solve the problem as follows.
              \begin{align*}
                  \min_{X\in\mathbb{R}^{m\times n}}\{\|A-X\|_F:\rank{X}\leq K\}.
              \end{align*}
              For simplicity, you can assume that all singular values of $A$ are different.

        \item \textbf{Programming Exercise} We provide you a grayscale image (``Alan\_Turing.jpg''). Suppose that $A$ is the data matrix of the image. We have $A\in\mathbb{R}^{512\times 512}$ and $r=\rank A=512$. In this exercise, you are expected to implement an image compression algorithm following the steps below. You can use your favorite programming language.
              \begin{enumerate}
                  \item Compute the SVD $A=U\Sigma V^\top=\sum_{i=1}^r\sigma_i\textbf{u}_i\textbf{v}_i^\top$, where $\sigma_1\ge \sigma_2\ge \dots\ge \sigma_r>0$ are the diagonal entries of $\Sigma$, $\textbf{u}_i$ is the $i$th column of $U$, and $\textbf{v}_i$ is the $i$th column of $V$.
                  \item Use the first $k$ $(k< r)$ terms of SVD to approximate the original image $A$. Then, we get the compressed images, of which the data matrices are $A_k=\sum_{i=1}^k\sigma_i\textbf{u}_i\textbf{v}_i^\top$. Compute $A_k$ for $k=2,4,8,16,32,64,128,256$.
                  \item Plot $A_k$ as images for all $k$.
              \end{enumerate}
              Please put the compressed images and their corresponding $k$ in this file.

    \end{enumerate}
\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item \begin{enumerate}
                  \item 
                  \item 
                  \item 
                  \item 
              \end{enumerate}
        \item \begin{enumerate}
                  \item  设 $\mathbf{a}_{i,*}$ 为 $A$ 的第 $i$ 行，则 $A^\top$ 的第 $i$ 列为 $\mathbf{a}_{i,*}^\top$.
                        由5.1 小题
                        $$A^\top A=\sum_{i=1}^{m}\mathbf{a}_{i,*}^\top \mathbf{a}_{i,*}$$
                        $$\tr \left( \mathbf{a}_{i,*}^\top \mathbf{a}_{i,*} \right)=\sum_{j=1}^{n} a_{i,j}^2$$
                        所以 $$\tr \left( A^\top A \right)=\sum_{i=1}^{m} \tr \left( \mathbf{a}_{i,*}^\top \mathbf{a}_{i,*} \right)=\sum_{i=1}^{m} \sum_{j=1}^{n} a_{i,j}^2$$
                        即
                        $$\|A\|_F^2=\tr \left(A^\top A \right)$$
                  \item 因为 $\langle \mathbf{a},\mathbf{b} \rangle=0$, 所以
                        $$A^\top B=\mathbf{0}, B^\top A=\mathbf{0}$$
                        $$\begin{aligned}
                                \|A+B\|_F^2
                                 & = \tr \left( \left(A+B\right)^\top \left(A+B\right) \right)                                                              \\
                                 & = \tr \left( \left(A^\top+B^\top\right) \left(A+B\right) \right)                                                         \\
                                 & = \tr \left( A^\top A \right) + \tr \left( A^\top B \right) + \tr \left( B^\top A \right)  + \tr \left( B^\top B \right) \\
                                 & =\tr \left( A^\top A \right) + \tr \left( B^\top B \right)                                                               \\
                                 & = \|A\|_F^2 +\|B\|_F^2
                            \end{aligned}$$
              \end{enumerate}
        \item
        \item
    \end{enumerate}
\end{solution}

\newpage

\begin{exercise}[PCA \textnormal{60pts}]
    Suppose that we have a set of data instances $\{\mathbf{x}_i\}_{i=1}^n\subset\mathbb{R}^d$. Let $\widetilde{X}\in\mathbb{R}^{d\times n}$ be the matrix whose $i^{th}$ column is $\mathbf{x}_i-\Bar{\mathbf{x}}$, where $\Bar{\mathbf{x}}$ is the sample mean, and $S$ be the sample variance matrix.

    \begin{enumerate}
        \item For $G\in\mathbb{R}^{d\times K}$, let us define
              \begin{align}\label{eqn:obj-PCA}
                  f(G) = \tr(G^{\top}SG).
              \end{align}
              Show that $f(GQ)=f(G)$ for any orthogonal matrix $Q\in\mathbb{R}^{K\times K}$.

        \item Please find $\mathbf{g}_1$ defined as follows by the Lagrange multiplier method.
              \begin{align}\label{eqn:PC1}
                  \mathbf{g}_1:=\argmax_{\mathbf{g}\in\mathbb{R}^d}\{f(\mathbf{g}):\|\mathbf{g}\|_2=1\},
              \end{align}
              where $f$ is defined by (\ref{eqn:obj-PCA}). Notice that, the vector $\mathbf{g}_1$ is the first principle component vector of the data.

        \item Please find $\mathbf{g}_2$ defined as follows by the Lagrange multiplier method.
              \begin{align*}
                  \mathbf{g}_2:=\argmax_{\mathbf{g}\in\mathbb{R}^d}\{f(\mathbf{g}):\|\mathbf{g}\|_2=1,\langle\mathbf{g},\mathbf{g}_1\rangle=0\},
              \end{align*}
              where $\mathbf{g}_1$ is given by (\ref{eqn:PC1}). Similar to $\mathbf{g}_1$, the vector $\mathbf{g}_2$ is the second principle component vector of the data.

        \item Please derive the first $K$ principle component vectors by repeating the above process.

        \item What is $f(\mathbf{g}_k)$, $k=1,\ldots,K$? What about their meaning?

        \item When the first $K$ principle component vectors are unique?
    \end{enumerate}

\end{exercise}

\begin{solution}
    \begin{enumerate}
        \item
        \item $$f(\mathbf{g})
                  =\tr \left( \mathbf{g}^\top S \mathbf{g}\right)
                  =\mathbf{g}^\top S \mathbf{g}$$
              $$\|\mathbf{g}\|_2=\mathbf{g}^\top \mathbf{g}=1$$
              令
              $$\nabla_\mathbf{g} f(\mathbf{g})=\left( S^\top +S \right) \mathbf{g}= 0$$
        \item
        \item
        \item
    \end{enumerate}
\end{solution}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
